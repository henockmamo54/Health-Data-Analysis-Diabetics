{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n# !pip install nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n# !pip install nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "# !pip install nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import pandas as pd\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom sklearn import metrics as m\\nfrom sklearn.datasets import make_classification\\nfrom imblearn.over_sampling import SMOTE  # doctest: +NORMALIZE_WHITESPACE\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.model_selection import train_test_split\\nfrom collections import Counter\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import VotingClassifier\\nimport xgboost as xgb\\nfrom sklearn.pipeline import make_pipeline\\nfrom mlxtend.feature_selection import ColumnSelector\\nfrom sklearn import model_selection\\nfrom mlxtend.classifier import StackingClassifier\\nfrom thundersvm import SVC as svmgpu\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nrandomseed = 7\\nnp.random.seed(randomseed)\";\n",
       "                var nbb_formatted_code = \"import pandas as pd\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nfrom sklearn import metrics as m\\nfrom sklearn.datasets import make_classification\\nfrom imblearn.over_sampling import SMOTE  # doctest: +NORMALIZE_WHITESPACE\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.model_selection import train_test_split\\nfrom collections import Counter\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import VotingClassifier\\nimport xgboost as xgb\\nfrom sklearn.pipeline import make_pipeline\\nfrom mlxtend.feature_selection import ColumnSelector\\nfrom sklearn import model_selection\\nfrom mlxtend.classifier import StackingClassifier\\nfrom thundersvm import SVC as svmgpu\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nrandomseed = 7\\nnp.random.seed(randomseed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics as m\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE  # doctest: +NORMALIZE_WHITESPACE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn import model_selection\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from thundersvm import SVC as svmgpu\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "randomseed = 7\n",
    "np.random.seed(randomseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185843, 19)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"x_original = pd.read_csv(\\\"../dataset/XLable_onlyDiabeticRemoved.txt\\\")\\n\\nconditions = [\\n    (x_original.L100800 < 100),\\n    (x_original.L100800 >= 100) & (x_original.L100800 < 126),\\n    (x_original.L100800 >= 126),\\n]\\nchoices = [0, 1, 2]\\nx_original[\\\"CLASS\\\"] = np.select(conditions, choices, default=0)\\nx_original = x_original[\\n    [\\n        \\\"Unnamed: 0\\\",\\n        \\\"L100800\\\",\\n        \\\"L104600\\\",\\n        \\\"L103000\\\",\\n        \\\"S000300\\\",\\n        \\\"L101700\\\",\\n        \\\"L100700\\\",\\n        \\\"FIELD_33\\\",\\n        \\\"FIELD_38\\\",\\n        \\\"FIELD_40\\\",\\n        \\\"FIELD_31\\\",\\n        \\\"SEX\\\",\\n        \\\"AGE\\\",  #'CLASS',\\n        \\\"FIELD_16\\\",\\n        \\\"FIELD_23\\\",\\n        \\\"FIELD_15\\\",\\n        \\\"FIELD_22\\\",\\n        \\\"FIELD_17\\\",\\n        \\\"FIELD_24\\\",\\n    ]\\n]\\n\\nprint(x_original.shape)\";\n",
       "                var nbb_formatted_code = \"x_original = pd.read_csv(\\\"../dataset/XLable_onlyDiabeticRemoved.txt\\\")\\n\\nconditions = [\\n    (x_original.L100800 < 100),\\n    (x_original.L100800 >= 100) & (x_original.L100800 < 126),\\n    (x_original.L100800 >= 126),\\n]\\nchoices = [0, 1, 2]\\nx_original[\\\"CLASS\\\"] = np.select(conditions, choices, default=0)\\nx_original = x_original[\\n    [\\n        \\\"Unnamed: 0\\\",\\n        \\\"L100800\\\",\\n        \\\"L104600\\\",\\n        \\\"L103000\\\",\\n        \\\"S000300\\\",\\n        \\\"L101700\\\",\\n        \\\"L100700\\\",\\n        \\\"FIELD_33\\\",\\n        \\\"FIELD_38\\\",\\n        \\\"FIELD_40\\\",\\n        \\\"FIELD_31\\\",\\n        \\\"SEX\\\",\\n        \\\"AGE\\\",  #'CLASS',\\n        \\\"FIELD_16\\\",\\n        \\\"FIELD_23\\\",\\n        \\\"FIELD_15\\\",\\n        \\\"FIELD_22\\\",\\n        \\\"FIELD_17\\\",\\n        \\\"FIELD_24\\\",\\n    ]\\n]\\n\\nprint(x_original.shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_original = pd.read_csv(\"../dataset/XLable_onlyDiabeticRemoved.txt\")\n",
    "\n",
    "conditions = [\n",
    "    (x_original.L100800 < 100),\n",
    "    (x_original.L100800 >= 100) & (x_original.L100800 < 126),\n",
    "    (x_original.L100800 >= 126),\n",
    "]\n",
    "choices = [0, 1, 2]\n",
    "x_original[\"CLASS\"] = np.select(conditions, choices, default=0)\n",
    "x_original = x_original[\n",
    "    [\n",
    "        \"Unnamed: 0\",\n",
    "        \"L100800\",\n",
    "        \"L104600\",\n",
    "        \"L103000\",\n",
    "        \"S000300\",\n",
    "        \"L101700\",\n",
    "        \"L100700\",\n",
    "        \"FIELD_33\",\n",
    "        \"FIELD_38\",\n",
    "        \"FIELD_40\",\n",
    "        \"FIELD_31\",\n",
    "        \"SEX\",\n",
    "        \"AGE\",  #'CLASS',\n",
    "        \"FIELD_16\",\n",
    "        \"FIELD_23\",\n",
    "        \"FIELD_15\",\n",
    "        \"FIELD_22\",\n",
    "        \"FIELD_17\",\n",
    "        \"FIELD_24\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(x_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185843, 2)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"y_original = pd.read_csv(\\\"../dataset/TargetLable_onlyDiabeticRemoved.txt\\\")\\n\\nconditions = [\\n    (y_original.L100800 < 100),\\n    (y_original.L100800 >= 100) & (y_original.L100800 < 126),\\n    (y_original.L100800 >= 126),\\n]\\n\\nchoices = [0, 1, 2]\\ny_original[\\\"CLASS\\\"] = np.select(conditions, choices, default=0)\\n\\ny_original = y_original[[\\\"Unnamed: 0\\\", \\\"CLASS\\\"]]\\n\\nprint(y_original.shape)\";\n",
       "                var nbb_formatted_code = \"y_original = pd.read_csv(\\\"../dataset/TargetLable_onlyDiabeticRemoved.txt\\\")\\n\\nconditions = [\\n    (y_original.L100800 < 100),\\n    (y_original.L100800 >= 100) & (y_original.L100800 < 126),\\n    (y_original.L100800 >= 126),\\n]\\n\\nchoices = [0, 1, 2]\\ny_original[\\\"CLASS\\\"] = np.select(conditions, choices, default=0)\\n\\ny_original = y_original[[\\\"Unnamed: 0\\\", \\\"CLASS\\\"]]\\n\\nprint(y_original.shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_original = pd.read_csv(\"../dataset/TargetLable_onlyDiabeticRemoved.txt\")\n",
    "\n",
    "conditions = [\n",
    "    (y_original.L100800 < 100),\n",
    "    (y_original.L100800 >= 100) & (y_original.L100800 < 126),\n",
    "    (y_original.L100800 >= 126),\n",
    "]\n",
    "\n",
    "choices = [0, 1, 2]\n",
    "y_original[\"CLASS\"] = np.select(conditions, choices, default=0)\n",
    "\n",
    "y_original = y_original[[\"Unnamed: 0\", \"CLASS\"]]\n",
    "\n",
    "print(y_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"data = pd.merge(\\n    x_original, y_original, how=\\\"inner\\\", left_on=\\\"Unnamed: 0\\\", right_on=\\\"Unnamed: 0\\\"\\n)\";\n",
       "                var nbb_formatted_code = \"data = pd.merge(\\n    x_original, y_original, how=\\\"inner\\\", left_on=\\\"Unnamed: 0\\\", right_on=\\\"Unnamed: 0\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.merge(\n",
    "    x_original, y_original, how=\"inner\", left_on=\"Unnamed: 0\", right_on=\"Unnamed: 0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169024, 20)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# filter the data set\\ndata = data[data.FIELD_16 != 1]  # exclude people who are diagnosed for (diabetes)\\ndata = data[data.FIELD_23 != 1]  # exclude people who are on medication for diabetes\\n\\ndata = data[\\n    data.FIELD_15 != 1\\n]  # exclude people who are diagnosed for (high blood pressure)\\ndata = data[\\n    data.FIELD_22 != 1\\n]  # exclude people who are on medication for high blood pressure\\n\\ndata = data[data.FIELD_17 != 1]  # exclude people who are diagnosed for hyperlipidemia\\ndata = data[\\n    data.FIELD_24 != 1\\n]  # exclude people who are on medication for hyperlipidemia\\n\\nprint(data.shape)\";\n",
       "                var nbb_formatted_code = \"# filter the data set\\ndata = data[data.FIELD_16 != 1]  # exclude people who are diagnosed for (diabetes)\\ndata = data[data.FIELD_23 != 1]  # exclude people who are on medication for diabetes\\n\\ndata = data[\\n    data.FIELD_15 != 1\\n]  # exclude people who are diagnosed for (high blood pressure)\\ndata = data[\\n    data.FIELD_22 != 1\\n]  # exclude people who are on medication for high blood pressure\\n\\ndata = data[data.FIELD_17 != 1]  # exclude people who are diagnosed for hyperlipidemia\\ndata = data[\\n    data.FIELD_24 != 1\\n]  # exclude people who are on medication for hyperlipidemia\\n\\nprint(data.shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filter the data set\n",
    "data = data[data.FIELD_16 != 1]  # exclude people who are diagnosed for (diabetes)\n",
    "data = data[data.FIELD_23 != 1]  # exclude people who are on medication for diabetes\n",
    "\n",
    "data = data[\n",
    "    data.FIELD_15 != 1\n",
    "]  # exclude people who are diagnosed for (high blood pressure)\n",
    "data = data[\n",
    "    data.FIELD_22 != 1\n",
    "]  # exclude people who are on medication for high blood pressure\n",
    "\n",
    "data = data[data.FIELD_17 != 1]  # exclude people who are diagnosed for hyperlipidemia\n",
    "data = data[\n",
    "    data.FIELD_24 != 1\n",
    "]  # exclude people who are on medication for hyperlipidemia\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56542, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L100800</th>\n",
       "      <th>L104600</th>\n",
       "      <th>L103000</th>\n",
       "      <th>S000300</th>\n",
       "      <th>L101700</th>\n",
       "      <th>L100700</th>\n",
       "      <th>FIELD_33</th>\n",
       "      <th>FIELD_38</th>\n",
       "      <th>FIELD_40</th>\n",
       "      <th>FIELD_31</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.0</td>\n",
       "      <td>5.28</td>\n",
       "      <td>41.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90.0</td>\n",
       "      <td>5.74</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>86.0</td>\n",
       "      <td>5.83</td>\n",
       "      <td>45.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>86.0</td>\n",
       "      <td>4.73</td>\n",
       "      <td>54.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>87.0</td>\n",
       "      <td>5.60</td>\n",
       "      <td>340.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    L100800  L104600  L103000  S000300  L101700  L100700  FIELD_33  FIELD_38  \\\n",
       "2      78.0     5.28     41.0     20.2     15.0      3.8       1.0       2.0   \n",
       "5      90.0     5.74     50.0     25.5     12.0      3.4       1.0       0.0   \n",
       "10     86.0     5.83     45.0     21.2     17.0      3.9       1.0       0.0   \n",
       "11     86.0     4.73     54.0     22.0     30.0      4.2       1.0       2.0   \n",
       "20     87.0     5.60    340.0     24.6     26.0      4.7       1.0       0.0   \n",
       "\n",
       "    FIELD_40  FIELD_31  SEX   AGE  CLASS  \n",
       "2        1.0       0.0  1.0  46.0      0  \n",
       "5        1.0       0.0  1.0  52.0      0  \n",
       "10       1.0       1.0  1.0  37.0      0  \n",
       "11       3.0       0.0  1.0  39.0      0  \n",
       "20       2.0       0.0  1.0  59.0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"data = data[\\n    [\\n        \\\"L100800\\\",\\n        \\\"L104600\\\",\\n        \\\"L103000\\\",\\n        \\\"S000300\\\",\\n        \\\"L101700\\\",\\n        \\\"L100700\\\",\\n        \\\"FIELD_33\\\",\\n        \\\"FIELD_38\\\",\\n        \\\"FIELD_40\\\",\\n        \\\"FIELD_31\\\",\\n        \\\"SEX\\\",\\n        \\\"AGE\\\",\\n        \\\"CLASS\\\",\\n    ]\\n]\\ndata = data.dropna()\\nprint(data.shape)\\ndata.head()\";\n",
       "                var nbb_formatted_code = \"data = data[\\n    [\\n        \\\"L100800\\\",\\n        \\\"L104600\\\",\\n        \\\"L103000\\\",\\n        \\\"S000300\\\",\\n        \\\"L101700\\\",\\n        \\\"L100700\\\",\\n        \\\"FIELD_33\\\",\\n        \\\"FIELD_38\\\",\\n        \\\"FIELD_40\\\",\\n        \\\"FIELD_31\\\",\\n        \\\"SEX\\\",\\n        \\\"AGE\\\",\\n        \\\"CLASS\\\",\\n    ]\\n]\\ndata = data.dropna()\\nprint(data.shape)\\ndata.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data[\n",
    "    [\n",
    "        \"L100800\",\n",
    "        \"L104600\",\n",
    "        \"L103000\",\n",
    "        \"S000300\",\n",
    "        \"L101700\",\n",
    "        \"L100700\",\n",
    "        \"FIELD_33\",\n",
    "        \"FIELD_38\",\n",
    "        \"FIELD_40\",\n",
    "        \"FIELD_31\",\n",
    "        \"SEX\",\n",
    "        \"AGE\",\n",
    "        \"CLASS\",\n",
    "    ]\n",
    "]\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downsample the majority class and upsample the minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045 17331 38166\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"diabetic = data[data.CLASS == 2]\\nprediabetic = data[data.CLASS == 1]\\nnormal = data[data.CLASS == 0]\\n\\nprint(diabetic.shape[0], prediabetic.shape[0], normal.shape[0])\";\n",
       "                var nbb_formatted_code = \"diabetic = data[data.CLASS == 2]\\nprediabetic = data[data.CLASS == 1]\\nnormal = data[data.CLASS == 0]\\n\\nprint(diabetic.shape[0], prediabetic.shape[0], normal.shape[0])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diabetic = data[data.CLASS == 2]\n",
    "prediabetic = data[data.CLASS == 1]\n",
    "normal = data[data.CLASS == 0]\n",
    "\n",
    "print(diabetic.shape[0], prediabetic.shape[0], normal.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"diabetic_test = diabetic.sample(200, random_state=randomseed)\\nprediabetic_test = prediabetic.sample(200, random_state=randomseed)\\nnormal_test = normal.sample(200, random_state=randomseed)\\ntest = pd.concat([diabetic_test, prediabetic_test, normal_test])\\n\\ndiabetic_train = diabetic.drop(diabetic_test.index)\\nprediabetic_train = prediabetic.drop(prediabetic_test.index)\\n# .sample(\\n#     10 * diabetic_train.shape[0], random_state=randomseed\\n# )\\nnormal_train = normal.drop(normal_test.index).sample(\\n    prediabetic_train.shape[0],\\n    random_state=randomseed\\n    #     10 * diabetic_train.shape[0], random_state=randomseed\\n)\\ntrain = pd.concat([diabetic_train, diabetic_train, prediabetic_train, normal_train])\";\n",
       "                var nbb_formatted_code = \"diabetic_test = diabetic.sample(200, random_state=randomseed)\\nprediabetic_test = prediabetic.sample(200, random_state=randomseed)\\nnormal_test = normal.sample(200, random_state=randomseed)\\ntest = pd.concat([diabetic_test, prediabetic_test, normal_test])\\n\\ndiabetic_train = diabetic.drop(diabetic_test.index)\\nprediabetic_train = prediabetic.drop(prediabetic_test.index)\\n# .sample(\\n#     10 * diabetic_train.shape[0], random_state=randomseed\\n# )\\nnormal_train = normal.drop(normal_test.index).sample(\\n    prediabetic_train.shape[0],\\n    random_state=randomseed\\n    #     10 * diabetic_train.shape[0], random_state=randomseed\\n)\\ntrain = pd.concat([diabetic_train, diabetic_train, prediabetic_train, normal_train])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diabetic_test = diabetic.sample(200, random_state=randomseed)\n",
    "prediabetic_test = prediabetic.sample(200, random_state=randomseed)\n",
    "normal_test = normal.sample(200, random_state=randomseed)\n",
    "test = pd.concat([diabetic_test, prediabetic_test, normal_test])\n",
    "\n",
    "diabetic_train = diabetic.drop(diabetic_test.index)\n",
    "prediabetic_train = prediabetic.drop(prediabetic_test.index)\n",
    "# .sample(\n",
    "#     10 * diabetic_train.shape[0], random_state=randomseed\n",
    "# )\n",
    "normal_train = normal.drop(normal_test.index).sample(\n",
    "    prediabetic_train.shape[0],\n",
    "    random_state=randomseed\n",
    "    #     10 * diabetic_train.shape[0], random_state=randomseed\n",
    ")\n",
    "train = pd.concat([diabetic_train, diabetic_train, prediabetic_train, normal_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"xtrain = train.iloc[:, :-1]\\nytrain = train.iloc[:, -1]\\nxtest = test.iloc[:, :-1]\\nytest = test.iloc[:, -1]\";\n",
       "                var nbb_formatted_code = \"xtrain = train.iloc[:, :-1]\\nytrain = train.iloc[:, -1]\\nxtest = test.iloc[:, :-1]\\nytest = test.iloc[:, -1]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtrain = train.iloc[:, :-1]\n",
    "ytrain = train.iloc[:, -1]\n",
    "xtest = test.iloc[:, :-1]\n",
    "ytest = test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"from sklearn.preprocessing import MinMaxScaler\\n\\nscaler = MinMaxScaler()\\nxtrain = scaler.fit_transform(xtrain)\\nxtest = scaler.transform(xtest)\";\n",
       "                var nbb_formatted_code = \"from sklearn.preprocessing import MinMaxScaler\\n\\nscaler = MinMaxScaler()\\nxtrain = scaler.fit_transform(xtrain)\\nxtest = scaler.transform(xtest)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "xtrain = scaler.fit_transform(xtrain)\n",
    "xtest = scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({2: 17131, 1: 17131, 0: 17131})\n",
      "17131 17131 17131\n",
      "(51393, 12) (51393,)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"from imblearn.over_sampling import SMOTE, SMOTENC  # doctest: +NORMALIZE_WHITESPACE\\n\\nrandomseed = 42\\n\\nsm = SMOTENC(\\n    random_state=randomseed,\\n    categorical_features=[6, 7, 8, 9, 10],\\n    sampling_strategy=\\\"minority\\\",\\n)\\nX_res, y_res = sm.fit_resample(xtrain, ytrain)\\n\\nprint(\\\"Resampled dataset shape %s\\\" % Counter(y_res))\\nprint(\\n    y_res[y_res == 0].shape[0], y_res[y_res == 1].shape[0], y_res[y_res == 2].shape[0]\\n)\\nprint(X_res.shape, y_res.shape)\\n\\nxtrain = X_res\\nytrain = y_res\";\n",
       "                var nbb_formatted_code = \"from imblearn.over_sampling import SMOTE, SMOTENC  # doctest: +NORMALIZE_WHITESPACE\\n\\nrandomseed = 42\\n\\nsm = SMOTENC(\\n    random_state=randomseed,\\n    categorical_features=[6, 7, 8, 9, 10],\\n    sampling_strategy=\\\"minority\\\",\\n)\\nX_res, y_res = sm.fit_resample(xtrain, ytrain)\\n\\nprint(\\\"Resampled dataset shape %s\\\" % Counter(y_res))\\nprint(\\n    y_res[y_res == 0].shape[0], y_res[y_res == 1].shape[0], y_res[y_res == 2].shape[0]\\n)\\nprint(X_res.shape, y_res.shape)\\n\\nxtrain = X_res\\nytrain = y_res\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE, SMOTENC  # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "randomseed = 42\n",
    "\n",
    "sm = SMOTENC(\n",
    "    random_state=randomseed,\n",
    "    categorical_features=[6, 7, 8, 9, 10],\n",
    "    sampling_strategy=\"minority\",\n",
    ")\n",
    "X_res, y_res = sm.fit_resample(xtrain, ytrain)\n",
    "\n",
    "print(\"Resampled dataset shape %s\" % Counter(y_res))\n",
    "print(\n",
    "    y_res[y_res == 0].shape[0], y_res[y_res == 1].shape[0], y_res[y_res == 2].shape[0]\n",
    ")\n",
    "print(X_res.shape, y_res.shape)\n",
    "\n",
    "xtrain = X_res\n",
    "ytrain = y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate the classifier models based on the selected  features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# generate the models based on the selected columns list and the ml classifiers\\n\\nweakmodles = []\\nestimators = []\";\n",
       "                var nbb_formatted_code = \"# generate the models based on the selected columns list and the ml classifiers\\n\\nweakmodles = []\\nestimators = []\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate the models based on the selected columns list and the ml classifiers\n",
    "\n",
    "weakmodles = []\n",
    "estimators = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"rf_model_12 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=10,\\n    max_features=\\\"log2\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=2,\\n    min_samples_split=12,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=-1,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=1,\\n    warm_start=False,\\n)\\n\\nxgb_model_12 = xgb.XGBClassifier(objective=\\\"binary:logistic\\\", random_state=randomseed)\\n\\nscv_model_12 = svmgpu(\\n    C=100,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_12 = SVC(\\n    C=100,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    #     gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    #     max_mem_size=-1,\\n    #     n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nweakmodles.append(rf_model_12)\\nweakmodles.append(xgb_model_12)\\nweakmodles.append(scv_model_12)\\n\\nestimators.append((\\\"rf_model_12\\\", rf_model_12))\\nestimators.append((\\\"xgb_model_12\\\", xgb_model_12))\\nestimators.append((\\\"scv_model_12\\\", scv_model_cpu_12))\\n\\n# estimators = [\\n#     (\\\"rf_model_12\\\", rf_model_12),\\n#     (\\\"xgb_model_12\\\", xgb_model_12),\\n#     (\\\"scv_model_12\\\", scv_model_12),\\n# ]\";\n",
       "                var nbb_formatted_code = \"rf_model_12 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=10,\\n    max_features=\\\"log2\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=2,\\n    min_samples_split=12,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=-1,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=1,\\n    warm_start=False,\\n)\\n\\nxgb_model_12 = xgb.XGBClassifier(objective=\\\"binary:logistic\\\", random_state=randomseed)\\n\\nscv_model_12 = svmgpu(\\n    C=100,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_12 = SVC(\\n    C=100,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    #     gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    #     max_mem_size=-1,\\n    #     n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nweakmodles.append(rf_model_12)\\nweakmodles.append(xgb_model_12)\\nweakmodles.append(scv_model_12)\\n\\nestimators.append((\\\"rf_model_12\\\", rf_model_12))\\nestimators.append((\\\"xgb_model_12\\\", xgb_model_12))\\nestimators.append((\\\"scv_model_12\\\", scv_model_cpu_12))\\n\\n# estimators = [\\n#     (\\\"rf_model_12\\\", rf_model_12),\\n#     (\\\"xgb_model_12\\\", xgb_model_12),\\n#     (\\\"scv_model_12\\\", scv_model_12),\\n# ]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_model_12 = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight=None,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=10,\n",
    "    max_features=\"log2\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=12,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    oob_score=False,\n",
    "    random_state=randomseed,\n",
    "    verbose=1,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "xgb_model_12 = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=randomseed)\n",
    "\n",
    "scv_model_12 = svmgpu(\n",
    "    C=100,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=False,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "scv_model_cpu_12 = SVC(\n",
    "    C=100,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    #     gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    #     max_mem_size=-1,\n",
    "    #     n_jobs=-1,\n",
    "    probability=False,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "weakmodles.append(rf_model_12)\n",
    "weakmodles.append(xgb_model_12)\n",
    "weakmodles.append(scv_model_12)\n",
    "\n",
    "estimators.append((\"rf_model_12\", rf_model_12))\n",
    "estimators.append((\"xgb_model_12\", xgb_model_12))\n",
    "estimators.append((\"scv_model_12\", scv_model_cpu_12))\n",
    "\n",
    "# estimators = [\n",
    "#     (\"rf_model_12\", rf_model_12),\n",
    "#     (\"xgb_model_12\", xgb_model_12),\n",
    "#     (\"scv_model_12\", scv_model_12),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2. 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"rf_model_5 = RandomForestClassifier(\\n    random_state=randomseed,\\n    n_estimators=100,\\n    max_depth=12,\\n    min_samples_split=2,\\n    min_samples_leaf=10,\\n    max_features=\\\"auto\\\",\\n)\\n\\nxgb_model_5 = xgb.XGBClassifier(objective=\\\"binary:logistic\\\", random_state=randomseed)\\n\\nscv_model_5 = svmgpu(\\n    C=70,\\n    cache_size=200,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovr\\\",\\n    degree=3,\\n    gamma=0.001,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=True,\\n    random_state=42,\\n    shrinking=True,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\nscv_model_cpu_5 = SVC(\\n    C=70,\\n    cache_size=200,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovr\\\",\\n    degree=3,\\n    gamma=0.001,\\n    #     gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    #     max_mem_size=-1,\\n    #     n_jobs=-1,\\n    probability=True,\\n    random_state=42,\\n    shrinking=True,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\nweakmodles.append(rf_model_5)\\nweakmodles.append(xgb_model_5)\\nweakmodles.append(scv_model_5)\\n\\nestimators.append((\\\"rf_model_5\\\", rf_model_5))\\nestimators.append((\\\"xgb_model_5\\\", xgb_model_5))\\nestimators.append((\\\"scv_model_5\\\", scv_model_cpu_5))\";\n",
       "                var nbb_formatted_code = \"rf_model_5 = RandomForestClassifier(\\n    random_state=randomseed,\\n    n_estimators=100,\\n    max_depth=12,\\n    min_samples_split=2,\\n    min_samples_leaf=10,\\n    max_features=\\\"auto\\\",\\n)\\n\\nxgb_model_5 = xgb.XGBClassifier(objective=\\\"binary:logistic\\\", random_state=randomseed)\\n\\nscv_model_5 = svmgpu(\\n    C=70,\\n    cache_size=200,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovr\\\",\\n    degree=3,\\n    gamma=0.001,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=True,\\n    random_state=42,\\n    shrinking=True,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\nscv_model_cpu_5 = SVC(\\n    C=70,\\n    cache_size=200,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovr\\\",\\n    degree=3,\\n    gamma=0.001,\\n    #     gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    #     max_mem_size=-1,\\n    #     n_jobs=-1,\\n    probability=True,\\n    random_state=42,\\n    shrinking=True,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\nweakmodles.append(rf_model_5)\\nweakmodles.append(xgb_model_5)\\nweakmodles.append(scv_model_5)\\n\\nestimators.append((\\\"rf_model_5\\\", rf_model_5))\\nestimators.append((\\\"xgb_model_5\\\", xgb_model_5))\\nestimators.append((\\\"scv_model_5\\\", scv_model_cpu_5))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_model_5 = RandomForestClassifier(\n",
    "    random_state=randomseed,\n",
    "    n_estimators=100,\n",
    "    max_depth=12,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=\"auto\",\n",
    ")\n",
    "\n",
    "xgb_model_5 = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=randomseed)\n",
    "\n",
    "scv_model_5 = svmgpu(\n",
    "    C=70,\n",
    "    cache_size=200,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovr\",\n",
    "    degree=3,\n",
    "    gamma=0.001,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    "    shrinking=True,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "scv_model_cpu_5 = SVC(\n",
    "    C=70,\n",
    "    cache_size=200,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovr\",\n",
    "    degree=3,\n",
    "    gamma=0.001,\n",
    "    #     gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    #     max_mem_size=-1,\n",
    "    #     n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    "    shrinking=True,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "weakmodles.append(rf_model_5)\n",
    "weakmodles.append(xgb_model_5)\n",
    "weakmodles.append(scv_model_5)\n",
    "\n",
    "estimators.append((\"rf_model_5\", rf_model_5))\n",
    "estimators.append((\"xgb_model_5\", xgb_model_5))\n",
    "estimators.append((\"scv_model_5\", scv_model_cpu_5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3. 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"top10colscomb = [\\n    (0, 1, 2, 3, 4, 5, 6, 9, 10, 11),\\n    (0, 1, 3, 4, 6, 7, 8, 9, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 6, 7, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 6, 8, 10, 11),\\n    (0, 1, 2, 3, 5, 6, 8, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 8, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 7, 9, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 8, 9, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 6, 7, 9, 11),\\n    (0, 1, 2, 3, 4, 6, 7, 9, 10, 11),\\n]\\n\\n# modles used in the 12 features set\\nrf_model_10 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=42,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_10 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=42,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_10 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=None,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_10 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=None,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top10colscomb[i]), rf_model_10))\\n    weakmodles.append(\\n        make_pipeline(ColumnSelector(cols=top10colscomb[i]), xgb_model_10)\\n    )\\n    weakmodles.append(\\n        make_pipeline(ColumnSelector(cols=top10colscomb[i]), scv_model_10)\\n    )\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_10\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top10colscomb[i]), rf_model_10)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_10\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top10colscomb[i]), xgb_model_10)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_10\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top10colscomb[i]), scv_model_cpu_10)),\\n        )\\n    )\";\n",
       "                var nbb_formatted_code = \"top10colscomb = [\\n    (0, 1, 2, 3, 4, 5, 6, 9, 10, 11),\\n    (0, 1, 3, 4, 6, 7, 8, 9, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 6, 7, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 6, 8, 10, 11),\\n    (0, 1, 2, 3, 5, 6, 8, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 8, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 7, 9, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 8, 9, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 6, 7, 9, 11),\\n    (0, 1, 2, 3, 4, 6, 7, 9, 10, 11),\\n]\\n\\n# modles used in the 12 features set\\nrf_model_10 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=42,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_10 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=42,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_10 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=None,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_10 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=None,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top10colscomb[i]), rf_model_10))\\n    weakmodles.append(\\n        make_pipeline(ColumnSelector(cols=top10colscomb[i]), xgb_model_10)\\n    )\\n    weakmodles.append(\\n        make_pipeline(ColumnSelector(cols=top10colscomb[i]), scv_model_10)\\n    )\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_10\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top10colscomb[i]), rf_model_10)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_10\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top10colscomb[i]), xgb_model_10)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_10\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top10colscomb[i]), scv_model_cpu_10)),\\n        )\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top10colscomb = [\n",
    "    (0, 1, 2, 3, 4, 5, 6, 9, 10, 11),\n",
    "    (0, 1, 3, 4, 6, 7, 8, 9, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 5, 6, 7, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 5, 6, 8, 10, 11),\n",
    "    (0, 1, 2, 3, 5, 6, 8, 9, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 6, 8, 9, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 6, 7, 9, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 5, 8, 9, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 5, 6, 7, 9, 11),\n",
    "    (0, 1, 2, 3, 4, 6, 7, 9, 10, 11),\n",
    "]\n",
    "\n",
    "# modles used in the 12 features set\n",
    "rf_model_10 = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight=None,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=12,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=10,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=100,\n",
    "    n_jobs=None,\n",
    "    oob_score=False,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "xgb_model_10 = xgb.XGBClassifier(\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    colsample_bytree=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_delta_step=0,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    missing=None,\n",
    "    n_estimators=100,\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    objective=\"multi:softprob\",\n",
    "    random_state=42,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=None,\n",
    "    silent=None,\n",
    "    subsample=1,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "scv_model_10 = svmgpu(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=False,\n",
    "    random_state=None,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "scv_model_cpu_10 = SVC(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,  # gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=None,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# generate the models based on the selected columns list and the ml classifiers\n",
    "for i in range(5):\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top10colscomb[i]), rf_model_10))\n",
    "    weakmodles.append(\n",
    "        make_pipeline(ColumnSelector(cols=top10colscomb[i]), xgb_model_10)\n",
    "    )\n",
    "    weakmodles.append(\n",
    "        make_pipeline(ColumnSelector(cols=top10colscomb[i]), scv_model_10)\n",
    "    )\n",
    "\n",
    "for i in range(5):\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"rf_model_10\" + str(i),\n",
    "                (make_pipeline(ColumnSelector(cols=top10colscomb[i]), rf_model_10)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"xgb_model_10\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top10colscomb[i]), xgb_model_10)),\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"scv_model_cpu_10\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top10colscomb[i]), scv_model_cpu_10)),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4. 9 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"topcols9comb = [\\n    (0, 1, 2, 3, 5, 6, 8, 10, 11),\\n    (0, 1, 2, 3, 4, 6, 9, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 6, 9, 11),\\n    (0, 1, 3, 4, 6, 7, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 8, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 8, 10, 11),\\n    (0, 1, 3, 4, 6, 7, 8, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 7, 8, 11),\\n    (0, 1, 3, 5, 6, 8, 9, 10, 11),\\n]\\n\\n# modles used in the 12 features set\\nrf_model_9 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_9 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_9 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_9 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), rf_model_9))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), xgb_model_9))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), scv_model_9))\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_9\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=topcols9comb[i]), rf_model_9)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_9\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=topcols9comb[i]), xgb_model_9)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_9\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=topcols9comb[i]), scv_model_cpu_9)),\\n        )\\n    )\";\n",
       "                var nbb_formatted_code = \"topcols9comb = [\\n    (0, 1, 2, 3, 5, 6, 8, 10, 11),\\n    (0, 1, 2, 3, 4, 6, 9, 10, 11),\\n    (0, 1, 2, 3, 4, 5, 6, 9, 11),\\n    (0, 1, 3, 4, 6, 7, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 8, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 8, 10, 11),\\n    (0, 1, 3, 4, 6, 7, 8, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 7, 8, 11),\\n    (0, 1, 3, 5, 6, 8, 9, 10, 11),\\n]\\n\\n# modles used in the 12 features set\\nrf_model_9 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_9 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_9 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_9 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), rf_model_9))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), xgb_model_9))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), scv_model_9))\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_9\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=topcols9comb[i]), rf_model_9)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_9\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=topcols9comb[i]), xgb_model_9)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_9\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=topcols9comb[i]), scv_model_cpu_9)),\\n        )\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topcols9comb = [\n",
    "    (0, 1, 2, 3, 5, 6, 8, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 6, 9, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 5, 6, 9, 11),\n",
    "    (0, 1, 3, 4, 6, 7, 9, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 8, 9, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 6, 9, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 6, 8, 10, 11),\n",
    "    (0, 1, 3, 4, 6, 7, 8, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 6, 7, 8, 11),\n",
    "    (0, 1, 3, 5, 6, 8, 9, 10, 11),\n",
    "]\n",
    "\n",
    "# modles used in the 12 features set\n",
    "rf_model_9 = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight=None,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=12,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=10,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=100,\n",
    "    n_jobs=None,\n",
    "    oob_score=False,\n",
    "    random_state=randomseed,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "xgb_model_9 = xgb.XGBClassifier(\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    colsample_bytree=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_delta_step=0,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    missing=None,\n",
    "    n_estimators=100,\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    objective=\"multi:softprob\",\n",
    "    random_state=randomseed,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=None,\n",
    "    silent=None,\n",
    "    subsample=1,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "scv_model_9 = svmgpu(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=False,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "scv_model_cpu_9 = SVC(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,  # gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "# generate the models based on the selected columns list and the ml classifiers\n",
    "for i in range(5):\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), rf_model_9))\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), xgb_model_9))\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=topcols9comb[i]), scv_model_9))\n",
    "\n",
    "for i in range(5):\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"rf_model_9\" + str(i),\n",
    "                (make_pipeline(ColumnSelector(cols=topcols9comb[i]), rf_model_9)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"xgb_model_9\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=topcols9comb[i]), xgb_model_9)),\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"scv_model_cpu_9\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=topcols9comb[i]), scv_model_cpu_9)),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5. 8 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"top8colscomb = [\\n    (0, 1, 2, 3, 5, 6, 10, 11),\\n    (0, 1, 2, 3, 4, 6, 10, 11),\\n    (0, 1, 2, 3, 4, 9, 10, 11),\\n    (0, 1, 2, 3, 5, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 9, 11),\\n    (0, 1, 3, 4, 6, 9, 10, 11),\\n    (0, 1, 3, 4, 8, 9, 10, 11),\\n    (0, 1, 2, 6, 8, 9, 10, 11),\\n    (0, 1, 2, 3, 5, 8, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 10, 11),\\n]\\n\\n# modles used in the 12 features set\\nrf_model_8 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_8 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_8 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=None,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_8 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), rf_model_8))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), xgb_model_8))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), scv_model_8))\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_8\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top8colscomb[i]), rf_model_8)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_8\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top8colscomb[i]), xgb_model_8)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_8\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top8colscomb[i]), scv_model_cpu_8)),\\n        )\\n    )\";\n",
       "                var nbb_formatted_code = \"top8colscomb = [\\n    (0, 1, 2, 3, 5, 6, 10, 11),\\n    (0, 1, 2, 3, 4, 6, 10, 11),\\n    (0, 1, 2, 3, 4, 9, 10, 11),\\n    (0, 1, 2, 3, 5, 9, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 9, 11),\\n    (0, 1, 3, 4, 6, 9, 10, 11),\\n    (0, 1, 3, 4, 8, 9, 10, 11),\\n    (0, 1, 2, 6, 8, 9, 10, 11),\\n    (0, 1, 2, 3, 5, 8, 10, 11),\\n    (0, 1, 3, 4, 5, 6, 10, 11),\\n]\\n\\n# modles used in the 12 features set\\nrf_model_8 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_8 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_8 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=None,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_8 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), rf_model_8))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), xgb_model_8))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), scv_model_8))\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_8\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top8colscomb[i]), rf_model_8)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_8\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top8colscomb[i]), xgb_model_8)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_8\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top8colscomb[i]), scv_model_cpu_8)),\\n        )\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top8colscomb = [\n",
    "    (0, 1, 2, 3, 5, 6, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 6, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 9, 10, 11),\n",
    "    (0, 1, 2, 3, 5, 9, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 6, 9, 11),\n",
    "    (0, 1, 3, 4, 6, 9, 10, 11),\n",
    "    (0, 1, 3, 4, 8, 9, 10, 11),\n",
    "    (0, 1, 2, 6, 8, 9, 10, 11),\n",
    "    (0, 1, 2, 3, 5, 8, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 6, 10, 11),\n",
    "]\n",
    "\n",
    "# modles used in the 12 features set\n",
    "rf_model_8 = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight=None,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=12,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=10,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=100,\n",
    "    n_jobs=None,\n",
    "    oob_score=False,\n",
    "    random_state=randomseed,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "xgb_model_8 = xgb.XGBClassifier(\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    colsample_bytree=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_delta_step=0,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    missing=None,\n",
    "    n_estimators=100,\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    objective=\"multi:softprob\",\n",
    "    random_state=randomseed,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=None,\n",
    "    silent=None,\n",
    "    subsample=1,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "scv_model_8 = svmgpu(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=False,\n",
    "    random_state=None,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "scv_model_cpu_8 = SVC(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,  # gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# generate the models based on the selected columns list and the ml classifiers\n",
    "for i in range(5):\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), rf_model_8))\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), xgb_model_8))\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top8colscomb[i]), scv_model_8))\n",
    "\n",
    "for i in range(5):\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"rf_model_8\" + str(i),\n",
    "                (make_pipeline(ColumnSelector(cols=top8colscomb[i]), rf_model_8)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"xgb_model_8\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top8colscomb[i]), xgb_model_8)),\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"scv_model_cpu_8\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top8colscomb[i]), scv_model_cpu_8)),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 7 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"top7colscomb = [\\n    (0, 1, 3, 5, 6, 10, 11),\\n    (0, 1, 3, 4, 5, 10, 11),\\n    (0, 1, 2, 3, 4, 10, 11),\\n    (0, 1, 2, 3, 5, 10, 11),\\n    (0, 1, 3, 6, 8, 10, 11),\\n    (0, 1, 6, 8, 9, 10, 11),\\n    (0, 1, 6, 7, 8, 9, 10),\\n    (0, 1, 2, 3, 8, 10, 11),\\n    (0, 1, 2, 6, 8, 10, 11),\\n    (0, 1, 3, 8, 9, 10, 11),\\n]\\n\\n\\nrf_model_7 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_7 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_7 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_7 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_7))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_7))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_7))\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_7\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_7)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_7\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_7)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_7\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_cpu_7)),\\n        )\\n    )\";\n",
       "                var nbb_formatted_code = \"top7colscomb = [\\n    (0, 1, 3, 5, 6, 10, 11),\\n    (0, 1, 3, 4, 5, 10, 11),\\n    (0, 1, 2, 3, 4, 10, 11),\\n    (0, 1, 2, 3, 5, 10, 11),\\n    (0, 1, 3, 6, 8, 10, 11),\\n    (0, 1, 6, 8, 9, 10, 11),\\n    (0, 1, 6, 7, 8, 9, 10),\\n    (0, 1, 2, 3, 8, 10, 11),\\n    (0, 1, 2, 6, 8, 10, 11),\\n    (0, 1, 3, 8, 9, 10, 11),\\n]\\n\\n\\nrf_model_7 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_7 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_7 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_7 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_7))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_7))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_7))\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_7\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_7)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_7\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_7)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_7\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_cpu_7)),\\n        )\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top7colscomb = [\n",
    "    (0, 1, 3, 5, 6, 10, 11),\n",
    "    (0, 1, 3, 4, 5, 10, 11),\n",
    "    (0, 1, 2, 3, 4, 10, 11),\n",
    "    (0, 1, 2, 3, 5, 10, 11),\n",
    "    (0, 1, 3, 6, 8, 10, 11),\n",
    "    (0, 1, 6, 8, 9, 10, 11),\n",
    "    (0, 1, 6, 7, 8, 9, 10),\n",
    "    (0, 1, 2, 3, 8, 10, 11),\n",
    "    (0, 1, 2, 6, 8, 10, 11),\n",
    "    (0, 1, 3, 8, 9, 10, 11),\n",
    "]\n",
    "\n",
    "\n",
    "rf_model_7 = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight=None,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=12,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=10,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=100,\n",
    "    n_jobs=None,\n",
    "    oob_score=False,\n",
    "    random_state=randomseed,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "xgb_model_7 = xgb.XGBClassifier(\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    colsample_bytree=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_delta_step=0,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    missing=None,\n",
    "    n_estimators=100,\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    objective=\"multi:softprob\",\n",
    "    random_state=randomseed,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=None,\n",
    "    silent=None,\n",
    "    subsample=1,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "scv_model_7 = svmgpu(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=False,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "scv_model_cpu_7 = SVC(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,  # gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "# generate the models based on the selected columns list and the ml classifiers\n",
    "for i in range(5):\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_7))\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_7))\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_7))\n",
    "\n",
    "for i in range(5):\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"rf_model_7\" + str(i),\n",
    "                (make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_7)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"xgb_model_7\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_7)),\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"scv_model_cpu_7\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_cpu_7)),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7. 6 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"top6colscomb = [\\n    (0, 1, 8, 9, 10),\\n    (0, 1, 6, 8, 10),\\n    (0, 1, 7, 9, 10),\\n    (0, 1, 2, 8, 10),\\n    (0, 1, 5, 8, 10),\\n    (0, 1, 5, 6, 11),\\n    (0, 1, 7, 8, 10),\\n    (0, 1, 9, 10, 11),\\n    (0, 1, 5, 10, 11),\\n    (0, 1, 2, 5, 11),\\n]\\n\\n# modles used in the 12 features set\\nrf_model_6 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_6 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_6 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_6 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), rf_model_6))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), xgb_model_6))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), scv_model_6))\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_6\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_6)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_6\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_6)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_6\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_cpu_6)),\\n        )\\n    )\";\n",
       "                var nbb_formatted_code = \"top6colscomb = [\\n    (0, 1, 8, 9, 10),\\n    (0, 1, 6, 8, 10),\\n    (0, 1, 7, 9, 10),\\n    (0, 1, 2, 8, 10),\\n    (0, 1, 5, 8, 10),\\n    (0, 1, 5, 6, 11),\\n    (0, 1, 7, 8, 10),\\n    (0, 1, 9, 10, 11),\\n    (0, 1, 5, 10, 11),\\n    (0, 1, 2, 5, 11),\\n]\\n\\n# modles used in the 12 features set\\nrf_model_6 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_6 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_6 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_6 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,  # gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), rf_model_6))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), xgb_model_6))\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), scv_model_6))\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_6\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_6)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_6\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_6)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_6\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_cpu_6)),\\n        )\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top6colscomb = [\n",
    "    (0, 1, 8, 9, 10),\n",
    "    (0, 1, 6, 8, 10),\n",
    "    (0, 1, 7, 9, 10),\n",
    "    (0, 1, 2, 8, 10),\n",
    "    (0, 1, 5, 8, 10),\n",
    "    (0, 1, 5, 6, 11),\n",
    "    (0, 1, 7, 8, 10),\n",
    "    (0, 1, 9, 10, 11),\n",
    "    (0, 1, 5, 10, 11),\n",
    "    (0, 1, 2, 5, 11),\n",
    "]\n",
    "\n",
    "# modles used in the 12 features set\n",
    "rf_model_6 = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight=None,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=12,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=10,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=100,\n",
    "    n_jobs=None,\n",
    "    oob_score=False,\n",
    "    random_state=randomseed,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "xgb_model_6 = xgb.XGBClassifier(\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    colsample_bytree=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_delta_step=0,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    missing=None,\n",
    "    n_estimators=100,\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    objective=\"multi:softprob\",\n",
    "    random_state=randomseed,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=None,\n",
    "    silent=None,\n",
    "    subsample=1,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "scv_model_6 = svmgpu(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=False,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "scv_model_cpu_6 = SVC(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,  # gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,  # max_mem_size=-1, n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "# generate the models based on the selected columns list and the ml classifiers\n",
    "for i in range(5):\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), rf_model_6))\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), xgb_model_6))\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top6colscomb[i]), scv_model_6))\n",
    "\n",
    "for i in range(5):\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"rf_model_6\" + str(i),\n",
    "                (make_pipeline(ColumnSelector(cols=top7colscomb[i]), rf_model_6)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"xgb_model_6\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), xgb_model_6)),\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"scv_model_cpu_6\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top7colscomb[i]), scv_model_cpu_6)),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8. 5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"top5colscomb = [\\n    (0, 1, 8, 9, 10),\\n    (0, 1, 6, 8, 10),\\n    (0, 1, 7, 9, 10),\\n    (0, 1, 2, 8, 10),\\n    (0, 1, 5, 8, 10),\\n    (0, 1, 5, 6, 11),\\n    (0, 1, 7, 8, 10),\\n    (0, 1, 9, 10, 11),\\n    (0, 1, 5, 10, 11),\\n    (0, 1, 2, 5, 11),\\n]\\n\\n\\nrf_model_5_2 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_5_2 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_5_2 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_5_2 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    #     gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    # max_mem_size=-1,   n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top5colscomb[i]), rf_model_5_2))\\n    weakmodles.append(\\n        make_pipeline(ColumnSelector(cols=top5colscomb[i]), xgb_model_5_2)\\n    )\\n    weakmodles.append(\\n        make_pipeline(ColumnSelector(cols=top5colscomb[i]), scv_model_5_2)\\n    )\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_52\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top5colscomb[i]), rf_model_5_2)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_52\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top5colscomb[i]), xgb_model_5_2)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_52\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top5colscomb[i]), scv_model_cpu_5_2)),\\n        )\\n    )\";\n",
       "                var nbb_formatted_code = \"top5colscomb = [\\n    (0, 1, 8, 9, 10),\\n    (0, 1, 6, 8, 10),\\n    (0, 1, 7, 9, 10),\\n    (0, 1, 2, 8, 10),\\n    (0, 1, 5, 8, 10),\\n    (0, 1, 5, 6, 11),\\n    (0, 1, 7, 8, 10),\\n    (0, 1, 9, 10, 11),\\n    (0, 1, 5, 10, 11),\\n    (0, 1, 2, 5, 11),\\n]\\n\\n\\nrf_model_5_2 = RandomForestClassifier(\\n    bootstrap=True,\\n    class_weight=None,\\n    criterion=\\\"gini\\\",\\n    max_depth=12,\\n    max_features=\\\"auto\\\",\\n    max_leaf_nodes=None,\\n    min_impurity_decrease=0.0,\\n    min_impurity_split=None,\\n    min_samples_leaf=10,\\n    min_samples_split=2,\\n    min_weight_fraction_leaf=0.0,\\n    n_estimators=100,\\n    n_jobs=None,\\n    oob_score=False,\\n    random_state=randomseed,\\n    verbose=0,\\n    warm_start=False,\\n)\\n\\nxgb_model_5_2 = xgb.XGBClassifier(\\n    base_score=0.5,\\n    booster=\\\"gbtree\\\",\\n    colsample_bylevel=1,\\n    colsample_bynode=1,\\n    colsample_bytree=1,\\n    gamma=0,\\n    learning_rate=0.1,\\n    max_delta_step=0,\\n    max_depth=3,\\n    min_child_weight=1,\\n    missing=None,\\n    n_estimators=100,\\n    n_jobs=1,\\n    nthread=None,\\n    objective=\\\"multi:softprob\\\",\\n    random_state=randomseed,\\n    reg_alpha=0,\\n    reg_lambda=1,\\n    scale_pos_weight=1,\\n    seed=None,\\n    silent=None,\\n    subsample=1,\\n    verbosity=1,\\n)\\n\\nscv_model_5_2 = svmgpu(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    max_mem_size=-1,\\n    n_jobs=-1,\\n    probability=False,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\nscv_model_cpu_5_2 = SVC(\\n    C=1000,\\n    cache_size=100,\\n    class_weight={},\\n    coef0=0.0,\\n    decision_function_shape=\\\"ovo\\\",\\n    degree=3,\\n    gamma=0.1,\\n    #     gpu_id=0,\\n    kernel=\\\"linear\\\",\\n    max_iter=-1,\\n    # max_mem_size=-1,   n_jobs=-1,\\n    probability=True,\\n    random_state=randomseed,\\n    shrinking=False,\\n    tol=0.001,\\n    verbose=False,\\n)\\n\\n\\n# generate the models based on the selected columns list and the ml classifiers\\nfor i in range(5):\\n    weakmodles.append(make_pipeline(ColumnSelector(cols=top5colscomb[i]), rf_model_5_2))\\n    weakmodles.append(\\n        make_pipeline(ColumnSelector(cols=top5colscomb[i]), xgb_model_5_2)\\n    )\\n    weakmodles.append(\\n        make_pipeline(ColumnSelector(cols=top5colscomb[i]), scv_model_5_2)\\n    )\\n\\nfor i in range(5):\\n    estimators.append(\\n        (\\n            (\\n                \\\"rf_model_52\\\" + str(i),\\n                (make_pipeline(ColumnSelector(cols=top5colscomb[i]), rf_model_5_2)),\\n            )\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"xgb_model_52\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top5colscomb[i]), xgb_model_5_2)),\\n        )\\n    )\\n    estimators.append(\\n        (\\n            \\\"scv_model_cpu_52\\\" + str(i),\\n            (make_pipeline(ColumnSelector(cols=top5colscomb[i]), scv_model_cpu_5_2)),\\n        )\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top5colscomb = [\n",
    "    (0, 1, 8, 9, 10),\n",
    "    (0, 1, 6, 8, 10),\n",
    "    (0, 1, 7, 9, 10),\n",
    "    (0, 1, 2, 8, 10),\n",
    "    (0, 1, 5, 8, 10),\n",
    "    (0, 1, 5, 6, 11),\n",
    "    (0, 1, 7, 8, 10),\n",
    "    (0, 1, 9, 10, 11),\n",
    "    (0, 1, 5, 10, 11),\n",
    "    (0, 1, 2, 5, 11),\n",
    "]\n",
    "\n",
    "\n",
    "rf_model_5_2 = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight=None,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=12,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=10,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=100,\n",
    "    n_jobs=None,\n",
    "    oob_score=False,\n",
    "    random_state=randomseed,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "xgb_model_5_2 = xgb.XGBClassifier(\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "    colsample_bytree=1,\n",
    "    gamma=0,\n",
    "    learning_rate=0.1,\n",
    "    max_delta_step=0,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    missing=None,\n",
    "    n_estimators=100,\n",
    "    n_jobs=1,\n",
    "    nthread=None,\n",
    "    objective=\"multi:softprob\",\n",
    "    random_state=randomseed,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=None,\n",
    "    silent=None,\n",
    "    subsample=1,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "scv_model_5_2 = svmgpu(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    max_mem_size=-1,\n",
    "    n_jobs=-1,\n",
    "    probability=False,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "scv_model_cpu_5_2 = SVC(\n",
    "    C=1000,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "    #     gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "    # max_mem_size=-1,   n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "# generate the models based on the selected columns list and the ml classifiers\n",
    "for i in range(5):\n",
    "    weakmodles.append(make_pipeline(ColumnSelector(cols=top5colscomb[i]), rf_model_5_2))\n",
    "    weakmodles.append(\n",
    "        make_pipeline(ColumnSelector(cols=top5colscomb[i]), xgb_model_5_2)\n",
    "    )\n",
    "    weakmodles.append(\n",
    "        make_pipeline(ColumnSelector(cols=top5colscomb[i]), scv_model_5_2)\n",
    "    )\n",
    "\n",
    "for i in range(5):\n",
    "    estimators.append(\n",
    "        (\n",
    "            (\n",
    "                \"rf_model_52\" + str(i),\n",
    "                (make_pipeline(ColumnSelector(cols=top5colscomb[i]), rf_model_5_2)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"xgb_model_52\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top5colscomb[i]), xgb_model_5_2)),\n",
    "        )\n",
    "    )\n",
    "    estimators.append(\n",
    "        (\n",
    "            \"scv_model_cpu_52\" + str(i),\n",
    "            (make_pipeline(ColumnSelector(cols=top5colscomb[i]), scv_model_cpu_5_2)),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"len(weakmodles)\";\n",
       "                var nbb_formatted_code = \"len(weakmodles)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(weakmodles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"len(estimators)\";\n",
       "                var nbb_formatted_code = \"len(estimators)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"sclf = StackingClassifier(classifiers=weakmodles,verbose=2,\\n                           meta_classifier=RandomForestClassifier(n_estimators=500))\";\n",
       "                var nbb_formatted_code = \"sclf = StackingClassifier(\\n    classifiers=weakmodles,\\n    verbose=2,\\n    meta_classifier=RandomForestClassifier(n_estimators=500),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sclf = StackingClassifier(classifiers=weakmodles,verbose=2,\n",
    "                           meta_classifier=RandomForestClassifier(n_estimators=500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 96 classifiers...\n",
      "Fitting classifier1: randomforestclassifier (1/96)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=10, max_features='log2', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=2, min_samples_split=12,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=42, verbose=1,\n",
      "                       warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier2: xgbclassifier (2/96)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "Fitting classifier3: svc (3/96)\n",
      "SVC(C=100, cache_size=100, class_weight={}, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma=0.1, gpu_id=0,\n",
      "    kernel='linear', max_iter=-1, max_mem_size=-1, n_jobs=-1, probability=False,\n",
      "    random_state=42, shrinking=False, tol=0.001, verbose=False)\n",
      "Fitting classifier4: randomforestclassifier (4/96)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=12, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=10, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "Fitting classifier5: xgbclassifier (5/96)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=42,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "Fitting classifier6: svc (6/96)\n",
      "SVC(C=70, cache_size=200, class_weight={}, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, gpu_id=0,\n",
      "    kernel='linear', max_iter=-1, max_mem_size=-1, n_jobs=-1, probability=True,\n",
      "    random_state=42, shrinking=True, tol=0.001, verbose=False)\n",
      "Fitting classifier7: pipeline (7/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier8: pipeline (8/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier9: pipeline (9/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier10: pipeline (10/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 6, 7, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier11: pipeline (11/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 6, 7, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier12: pipeline (12/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 6, 7, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier13: pipeline (13/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 7, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier14: pipeline (14/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 7, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier15: pipeline (15/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 7, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier16: pipeline (16/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 8, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier17: pipeline (17/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 8, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier18: pipeline (18/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 8, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier19: pipeline (19/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier20: pipeline (20/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier21: pipeline (21/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier22: pipeline (22/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 8, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier23: pipeline (23/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 8, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier24: pipeline (24/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 8, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier25: pipeline (25/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 6, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier26: pipeline (26/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 6, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier27: pipeline (27/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 6, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier28: pipeline (28/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 9, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier29: pipeline (29/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 9, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier30: pipeline (30/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 5, 6, 9, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier31: pipeline (31/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 6, 7, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier32: pipeline (32/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 6, 7, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier33: pipeline (33/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 6, 7, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier34: pipeline (34/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier35: pipeline (35/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier36: pipeline (36/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 8, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier37: pipeline (37/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier38: pipeline (38/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier39: pipeline (39/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 6, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier40: pipeline (40/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 6, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier41: pipeline (41/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 6, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier42: pipeline (42/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 6, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier43: pipeline (43/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier44: pipeline (44/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier45: pipeline (45/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier46: pipeline (46/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier47: pipeline (47/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier48: pipeline (48/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 9, 10, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier49: pipeline (49/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 6, 9, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier50: pipeline (50/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 6, 9, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier51: pipeline (51/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 6, 9, 11),\n",
      "                                drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=None,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier52: pipeline (52/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 5, 6, 10, 11), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier53: pipeline (53/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 5, 6, 10, 11), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier54: pipeline (54/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 5, 6, 10, 11), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier55: pipeline (55/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 10, 11), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier56: pipeline (56/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 10, 11), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier57: pipeline (57/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 4, 5, 10, 11), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier58: pipeline (58/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 10, 11), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier59: pipeline (59/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 10, 11), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier60: pipeline (60/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 4, 10, 11), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier61: pipeline (61/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 10, 11), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier62: pipeline (62/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 10, 11), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier63: pipeline (63/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 3, 5, 10, 11), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier64: pipeline (64/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 6, 8, 10, 11), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier65: pipeline (65/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 6, 8, 10, 11), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier66: pipeline (66/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 3, 6, 8, 10, 11), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier67: pipeline (67/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 8, 9, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier68: pipeline (68/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 8, 9, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier69: pipeline (69/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 8, 9, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier70: pipeline (70/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 6, 8, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier71: pipeline (71/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 6, 8, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier72: pipeline (72/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 6, 8, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier73: pipeline (73/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 7, 9, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier74: pipeline (74/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 7, 9, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier75: pipeline (75/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 7, 9, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier76: pipeline (76/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 8, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier77: pipeline (77/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 8, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier78: pipeline (78/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 8, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier79: pipeline (79/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 5, 8, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier80: pipeline (80/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 5, 8, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier81: pipeline (81/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 5, 8, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier82: pipeline (82/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 8, 9, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier83: pipeline (83/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 8, 9, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier84: pipeline (84/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 8, 9, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier85: pipeline (85/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 6, 8, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier86: pipeline (86/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 6, 8, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier87: pipeline (87/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 6, 8, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier88: pipeline (88/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 7, 9, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier89: pipeline (89/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 7, 9, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier90: pipeline (90/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 7, 9, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier91: pipeline (91/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 8, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier92: pipeline (92/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 8, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier93: pipeline (93/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 2, 8, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n",
      "Fitting classifier94: pipeline (94/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 5, 8, 10), drop_axis=False)),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                        criterion='gini', max_depth=12,\n",
      "                                        max_features='auto',\n",
      "                                        max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=10,\n",
      "                                        min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=42,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier95: pipeline (95/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 5, 8, 10), drop_axis=False)),\n",
      "                ('xgbclassifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
      "                               max_delta_step=0, max_depth=3,\n",
      "                               min_child_weight=1, missing=None,\n",
      "                               n_estimators=100, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=42,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "Fitting classifier96: pipeline (96/96)\n",
      "Pipeline(memory=None,\n",
      "         steps=[('columnselector',\n",
      "                 ColumnSelector(cols=(0, 1, 5, 8, 10), drop_axis=False)),\n",
      "                ('svc',\n",
      "                 SVC(C=1000, cache_size=100, class_weight={}, coef0=0.0,\n",
      "                     decision_function_shape='ovo', degree=3, gamma=0.1,\n",
      "                     gpu_id=0, kernel='linear', max_iter=-1, max_mem_size=-1,\n",
      "                     n_jobs=-1, probability=False, random_state=42,\n",
      "                     shrinking=False, tol=0.001, verbose=False))],\n",
      "         verbose=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(average_probas=False,\n",
       "                   classifiers=[RandomForestClassifier(bootstrap=True,\n",
       "                                                       class_weight=None,\n",
       "                                                       criterion='gini',\n",
       "                                                       max_depth=10,\n",
       "                                                       max_features='log2',\n",
       "                                                       max_leaf_nodes=None,\n",
       "                                                       min_impurity_decrease=0.0,\n",
       "                                                       min_impurity_split=None,\n",
       "                                                       min_samples_leaf=2,\n",
       "                                                       min_samples_split=12,\n",
       "                                                       min_weight_fraction_leaf=0.0,\n",
       "                                                       n_estimators=100,\n",
       "                                                       n_jobs=-1,\n",
       "                                                       oob_score=False,\n",
       "                                                       random_st...\n",
       "                                                          max_features='auto',\n",
       "                                                          max_leaf_nodes=None,\n",
       "                                                          min_impurity_decrease=0.0,\n",
       "                                                          min_impurity_split=None,\n",
       "                                                          min_samples_leaf=1,\n",
       "                                                          min_samples_split=2,\n",
       "                                                          min_weight_fraction_leaf=0.0,\n",
       "                                                          n_estimators=500,\n",
       "                                                          n_jobs=None,\n",
       "                                                          oob_score=False,\n",
       "                                                          random_state=None,\n",
       "                                                          verbose=0,\n",
       "                                                          warm_start=False),\n",
       "                   store_train_meta_features=False, use_clones=True,\n",
       "                   use_features_in_secondary=False, use_probas=False,\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"sclf.fit(xtrain, ytrain)\";\n",
       "                var nbb_formatted_code = \"sclf.fit(xtrain, ytrain)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sclf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy \t 0.725\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normal</th>\n",
       "      <th>Prediabetes</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>145</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediabetes</th>\n",
       "      <td>60</td>\n",
       "      <td>129</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Normal  Prediabetes  diabetes\n",
       "Normal          145           55         0\n",
       "Prediabetes      60          129        11\n",
       "diabetes          3           36       161"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"ypred = sclf.predict((xtest))\\n\\nprint(\\\"accuracy \\\\t\\\", m.accuracy_score(ytest, ypred))\\nconfmatrx = pd.DataFrame(\\n    m.confusion_matrix(ytest, ypred),\\n    columns=[\\\"Normal\\\", \\\"Prediabetes\\\", \\\"diabetes\\\"],\\n    index=[\\\"Normal\\\", \\\"Prediabetes\\\", \\\"diabetes\\\"],\\n)\\nconfmatrx.head()\";\n",
       "                var nbb_formatted_code = \"ypred = sclf.predict((xtest))\\n\\nprint(\\\"accuracy \\\\t\\\", m.accuracy_score(ytest, ypred))\\nconfmatrx = pd.DataFrame(\\n    m.confusion_matrix(ytest, ypred),\\n    columns=[\\\"Normal\\\", \\\"Prediabetes\\\", \\\"diabetes\\\"],\\n    index=[\\\"Normal\\\", \\\"Prediabetes\\\", \\\"diabetes\\\"],\\n)\\nconfmatrx.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ypred = sclf.predict((xtest))\n",
    "\n",
    "print(\"accuracy \\t\", m.accuracy_score(ytest, ypred))\n",
    "confmatrx = pd.DataFrame(\n",
    "    m.confusion_matrix(ytest, ypred),\n",
    "    columns=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "    index=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    ")\n",
    "confmatrx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.72      0.71       200\n",
      "           1       0.59      0.65      0.61       200\n",
      "           2       0.94      0.81      0.87       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.72      0.73       600\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"print(classification_report(ytest, ypred))\";\n",
       "                var nbb_formatted_code = \"print(classification_report(ytest, ypred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the performance of each model of the sclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"# for i in range(len(sclf.clfs_)):\\n#     ypred = sclf.clfs_[i].predict((xtest))\\n#     print(type(sclf.clfs_[i]), \\\"accuracy \\\\t\\\", m.accuracy_score(ytest, ypred))\";\n",
       "                var nbb_formatted_code = \"# for i in range(len(sclf.clfs_)):\\n#     ypred = sclf.clfs_[i].predict((xtest))\\n#     print(type(sclf.clfs_[i]), \\\"accuracy \\\\t\\\", m.accuracy_score(ytest, ypred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for i in range(len(sclf.clfs_)):\n",
    "#     ypred = sclf.clfs_[i].predict((xtest))\n",
    "#     print(type(sclf.clfs_[i]), \"accuracy \\t\", m.accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'> accuracy \t 0.73\n",
      "Accuracy =  0.73\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      56          124        20\n",
      "diabetes          3           33       164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       200\n",
      "           1       0.60      0.62      0.61       200\n",
      "           2       0.89      0.82      0.85       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.73      0.73       600\n",
      "\n",
      "0\n",
      "<class 'xgboost.sklearn.XGBClassifier'> accuracy \t 0.735\n",
      "Accuracy =  0.735\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           48         1\n",
      "Prediabetes      59          116        25\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73       200\n",
      "           1       0.62      0.58      0.60       200\n",
      "           2       0.87      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.73      0.73      0.73       600\n",
      "weighted avg       0.73      0.73      0.73       600\n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'thundersvm.thundersvm.SVC'> accuracy \t 0.735\n",
      "Accuracy =  0.735\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      55          119        26\n",
      "diabetes          4           24       172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       200\n",
      "           1       0.62      0.59      0.61       200\n",
      "           2       0.86      0.86      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.73      0.73      0.73       600\n",
      "weighted avg       0.73      0.73      0.73       600\n",
      "\n",
      "2\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'> accuracy \t 0.7333333333333333\n",
      "Accuracy =  0.7333333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           51         1\n",
      "Prediabetes      55          127        18\n",
      "diabetes          4           31       165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73       200\n",
      "           1       0.61      0.64      0.62       200\n",
      "           2       0.90      0.82      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.74       600\n",
      "weighted avg       0.74      0.73      0.74       600\n",
      "\n",
      "3\n",
      "<class 'xgboost.sklearn.XGBClassifier'> accuracy \t 0.735\n",
      "Accuracy =  0.735\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           48         1\n",
      "Prediabetes      59          116        25\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73       200\n",
      "           1       0.62      0.58      0.60       200\n",
      "           2       0.87      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.73      0.73      0.73       600\n",
      "weighted avg       0.73      0.73      0.73       600\n",
      "\n",
      "4\n",
      "<class 'thundersvm.thundersvm.SVC'> accuracy \t 0.7366666666666667\n",
      "Accuracy =  0.7366666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           51         1\n",
      "Prediabetes      53          122        25\n",
      "diabetes          3           25       172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73       200\n",
      "           1       0.62      0.61      0.61       200\n",
      "           2       0.87      0.86      0.86       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "5\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7383333333333333\n",
      "Accuracy =  0.7383333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      54          128        18\n",
      "diabetes          4           30       166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       200\n",
      "           1       0.61      0.64      0.63       200\n",
      "           2       0.90      0.83      0.86       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "6\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.745\n",
      "Accuracy =  0.745\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      55          119        26\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.77      0.74       200\n",
      "           1       0.64      0.59      0.61       200\n",
      "           2       0.87      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "7\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.555\n",
      "Accuracy =  0.555\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           41         9\n",
      "Prediabetes      54          128        18\n",
      "diabetes          4          141        55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.41      0.64      0.50       200\n",
      "           2       0.67      0.28      0.39       200\n",
      "\n",
      "    accuracy                           0.56       600\n",
      "   macro avg       0.60      0.56      0.54       600\n",
      "weighted avg       0.60      0.56      0.54       600\n",
      "\n",
      "8\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7416666666666667\n",
      "Accuracy =  0.7416666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      54          131        15\n",
      "diabetes          3           33       164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.61      0.66      0.63       200\n",
      "           2       0.92      0.82      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.75      0.74      0.75       600\n",
      "weighted avg       0.75      0.74      0.75       600\n",
      "\n",
      "9\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.745\n",
      "Accuracy =  0.745\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           48         1\n",
      "Prediabetes      54          121        25\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       200\n",
      "           1       0.64      0.60      0.62       200\n",
      "           2       0.87      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "10\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7316666666666667\n",
      "Accuracy =  0.7316666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           48         1\n",
      "Prediabetes      56          116        28\n",
      "diabetes          4           24       172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.73       200\n",
      "           1       0.62      0.58      0.60       200\n",
      "           2       0.86      0.86      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.73      0.73      0.73       600\n",
      "weighted avg       0.73      0.73      0.73       600\n",
      "\n",
      "11\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.735\n",
      "Accuracy =  0.735\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           52         0\n",
      "Prediabetes      55          129        16\n",
      "diabetes          4           32       164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73       200\n",
      "           1       0.61      0.65      0.62       200\n",
      "           2       0.91      0.82      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.74       600\n",
      "weighted avg       0.74      0.73      0.74       600\n",
      "\n",
      "12\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7433333333333333\n",
      "Accuracy =  0.7433333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           49         0\n",
      "Prediabetes      56          119        25\n",
      "diabetes          4           20       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.73       200\n",
      "           1       0.63      0.59      0.61       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "13\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.735\n",
      "Accuracy =  0.735\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      56          117        27\n",
      "diabetes          3           23       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       200\n",
      "           1       0.62      0.58      0.60       200\n",
      "           2       0.86      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.73      0.73      0.73       600\n",
      "weighted avg       0.73      0.73      0.73       600\n",
      "\n",
      "14\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7283333333333334\n",
      "Accuracy =  0.7283333333333334\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           50         1\n",
      "Prediabetes      56          125        19\n",
      "diabetes          4           33       163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73       200\n",
      "           1       0.60      0.62      0.61       200\n",
      "           2       0.89      0.81      0.85       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.73      0.73      0.73       600\n",
      "weighted avg       0.73      0.73      0.73       600\n",
      "\n",
      "15\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7433333333333333\n",
      "Accuracy =  0.7433333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      55          119        26\n",
      "diabetes          3           23       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.74       200\n",
      "           1       0.63      0.59      0.61       200\n",
      "           2       0.87      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "16\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7383333333333333\n",
      "Accuracy =  0.7383333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      55          119        26\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       200\n",
      "           1       0.63      0.59      0.61       200\n",
      "           2       0.87      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "17\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.73\n",
      "Accuracy =  0.73\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           51         1\n",
      "Prediabetes      57          127        16\n",
      "diabetes          3           34       163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73       200\n",
      "           1       0.60      0.64      0.62       200\n",
      "           2       0.91      0.81      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.73      0.73       600\n",
      "\n",
      "18\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7433333333333333\n",
      "Accuracy =  0.7433333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           46         1\n",
      "Prediabetes      54          119        27\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.74       200\n",
      "           1       0.64      0.59      0.61       200\n",
      "           2       0.86      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "19\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7383333333333333\n",
      "Accuracy =  0.7383333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      54          119        27\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.63      0.59      0.61       200\n",
      "           2       0.86      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "20\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7266666666666667\n",
      "Accuracy =  0.7266666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          146           53         1\n",
      "Prediabetes      56          127        17\n",
      "diabetes          4           33       163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72       200\n",
      "           1       0.60      0.64      0.62       200\n",
      "           2       0.90      0.81      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.73      0.73       600\n",
      "\n",
      "21\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.75\n",
      "Accuracy =  0.75\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      53          122        25\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       200\n",
      "           1       0.64      0.61      0.63       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "22\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7383333333333333\n",
      "Accuracy =  0.7383333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           50         1\n",
      "Prediabetes      53          120        27\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       200\n",
      "           1       0.62      0.60      0.61       200\n",
      "           2       0.86      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "23\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7333333333333333\n",
      "Accuracy =  0.7333333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           52         0\n",
      "Prediabetes      57          126        17\n",
      "diabetes          4           30       166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72       200\n",
      "           1       0.61      0.63      0.62       200\n",
      "           2       0.91      0.83      0.87       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.74       600\n",
      "weighted avg       0.74      0.73      0.74       600\n",
      "\n",
      "24\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      54          121        25\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.74       200\n",
      "           1       0.64      0.60      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "25\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7416666666666667\n",
      "Accuracy =  0.7416666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           48         1\n",
      "Prediabetes      53          119        28\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74       200\n",
      "           1       0.63      0.59      0.61       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "26\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7366666666666667\n",
      "Accuracy =  0.7366666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           52         0\n",
      "Prediabetes      57          129        14\n",
      "diabetes          4           31       165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72       200\n",
      "           1       0.61      0.65      0.63       200\n",
      "           2       0.92      0.82      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.75      0.74      0.74       600\n",
      "weighted avg       0.75      0.74      0.74       600\n",
      "\n",
      "27\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7466666666666667\n",
      "Accuracy =  0.7466666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152           48         0\n",
      "Prediabetes      53          121        26\n",
      "diabetes          3           22       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75       200\n",
      "           1       0.63      0.60      0.62       200\n",
      "           2       0.87      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.74      0.75      0.75       600\n",
      "weighted avg       0.74      0.75      0.75       600\n",
      "\n",
      "28\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.745\n",
      "Accuracy =  0.745\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           46         1\n",
      "Prediabetes      54          118        28\n",
      "diabetes          3           21       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       200\n",
      "           1       0.64      0.59      0.61       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "29\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.73\n",
      "Accuracy =  0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           52         0\n",
      "Prediabetes      57          126        17\n",
      "diabetes          4           32       164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72       200\n",
      "           1       0.60      0.63      0.61       200\n",
      "           2       0.91      0.82      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.73      0.73       600\n",
      "\n",
      "30\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7466666666666667\n",
      "Accuracy =  0.7466666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      54          123        23\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.64      0.61      0.63       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "31\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.74\n",
      "Accuracy =  0.74\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           48         1\n",
      "Prediabetes      55          117        28\n",
      "diabetes          4           20       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       200\n",
      "           1       0.63      0.58      0.61       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "32\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7316666666666667\n",
      "Accuracy =  0.7316666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152           48         0\n",
      "Prediabetes      57          124        19\n",
      "diabetes          4           33       163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.74       200\n",
      "           1       0.60      0.62      0.61       200\n",
      "           2       0.90      0.81      0.85       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.73      0.73       600\n",
      "\n",
      "33\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.745\n",
      "Accuracy =  0.745\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      56          119        25\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.77      0.74       200\n",
      "           1       0.64      0.59      0.61       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "34\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7333333333333333\n",
      "Accuracy =  0.7333333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      55          117        28\n",
      "diabetes          4           23       173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       200\n",
      "           1       0.62      0.58      0.60       200\n",
      "           2       0.86      0.86      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.73      0.73      0.73       600\n",
      "weighted avg       0.73      0.73      0.73       600\n",
      "\n",
      "35\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7366666666666667\n",
      "Accuracy =  0.7366666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           52         0\n",
      "Prediabetes      54          130        16\n",
      "diabetes          3           33       164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       200\n",
      "           1       0.60      0.65      0.63       200\n",
      "           2       0.91      0.82      0.86       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.75      0.74      0.74       600\n",
      "weighted avg       0.75      0.74      0.74       600\n",
      "\n",
      "36\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7533333333333333\n",
      "Accuracy =  0.7533333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          154           46         0\n",
      "Prediabetes      53          122        25\n",
      "diabetes          4           20       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       200\n",
      "           1       0.65      0.61      0.63       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "37\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.74\n",
      "Accuracy =  0.74\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           51         1\n",
      "Prediabetes      52          120        28\n",
      "diabetes          4           20       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73       200\n",
      "           1       0.63      0.60      0.61       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "38\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.74\n",
      "Accuracy =  0.74\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           52         0\n",
      "Prediabetes      56          126        18\n",
      "diabetes          4           26       170\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73       200\n",
      "           1       0.62      0.63      0.62       200\n",
      "           2       0.90      0.85      0.88       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "39\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.75\n",
      "Accuracy =  0.75\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152           48         0\n",
      "Prediabetes      53          122        25\n",
      "diabetes          3           21       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75       200\n",
      "           1       0.64      0.61      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "40\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7433333333333333\n",
      "Accuracy =  0.7433333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      53          121        26\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.63      0.60      0.62       200\n",
      "           2       0.87      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "41\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7333333333333333\n",
      "Accuracy =  0.7333333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           51         1\n",
      "Prediabetes      57          126        17\n",
      "diabetes          4           30       166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72       200\n",
      "           1       0.61      0.63      0.62       200\n",
      "           2       0.90      0.83      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.74       600\n",
      "weighted avg       0.74      0.73      0.74       600\n",
      "\n",
      "42\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152           48         0\n",
      "Prediabetes      54          122        24\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       200\n",
      "           1       0.64      0.61      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "43\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7383333333333333\n",
      "Accuracy =  0.7383333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      55          119        26\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       200\n",
      "           1       0.63      0.59      0.61       200\n",
      "           2       0.87      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "44\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.73\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           52         0\n",
      "Prediabetes      59          125        16\n",
      "diabetes          4           31       165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       200\n",
      "           1       0.60      0.62      0.61       200\n",
      "           2       0.91      0.82      0.87       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.73      0.73       600\n",
      "\n",
      "45\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      54          121        25\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.74       200\n",
      "           1       0.64      0.60      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "46\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.735\n",
      "Accuracy =  0.735\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           50         1\n",
      "Prediabetes      54          117        29\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       200\n",
      "           1       0.62      0.58      0.60       200\n",
      "           2       0.85      0.88      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.73      0.73      0.73       600\n",
      "weighted avg       0.73      0.73      0.73       600\n",
      "\n",
      "47\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7266666666666667\n",
      "Accuracy =  0.7266666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          148           52         0\n",
      "Prediabetes      59          125        16\n",
      "diabetes          4           33       163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       200\n",
      "           1       0.60      0.62      0.61       200\n",
      "           2       0.91      0.81      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.73      0.73       600\n",
      "\n",
      "48\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      54          121        25\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.74       200\n",
      "           1       0.64      0.60      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "49\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.75\n",
      "Accuracy =  0.75\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           46         1\n",
      "Prediabetes      53          120        27\n",
      "diabetes          3           20       177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       200\n",
      "           1       0.65      0.60      0.62       200\n",
      "           2       0.86      0.89      0.87       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "50\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7383333333333333\n",
      "Accuracy =  0.7383333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      52          130        18\n",
      "diabetes          4           33       163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       200\n",
      "           1       0.61      0.65      0.63       200\n",
      "           2       0.90      0.81      0.86       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.75      0.74      0.74       600\n",
      "weighted avg       0.75      0.74      0.74       600\n",
      "\n",
      "51\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.75\n",
      "Accuracy =  0.75\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          154           46         0\n",
      "Prediabetes      54          121        25\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       200\n",
      "           1       0.64      0.60      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "52\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7433333333333333\n",
      "Accuracy =  0.7433333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      52          120        28\n",
      "diabetes          4           20       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       200\n",
      "           1       0.63      0.60      0.62       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "53\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.74\n",
      "Accuracy =  0.74\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      55          129        16\n",
      "diabetes          4           31       165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       200\n",
      "           1       0.61      0.65      0.63       200\n",
      "           2       0.91      0.82      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.75      0.74      0.74       600\n",
      "weighted avg       0.75      0.74      0.74       600\n",
      "\n",
      "54\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.75\n",
      "Accuracy =  0.75\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      54          121        25\n",
      "diabetes          3           21       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       200\n",
      "           1       0.64      0.60      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "55\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7416666666666667\n",
      "Accuracy =  0.7416666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      54          120        26\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.63      0.60      0.62       200\n",
      "           2       0.87      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "56\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7366666666666667\n",
      "Accuracy =  0.7366666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      57          126        17\n",
      "diabetes          4           30       166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73       200\n",
      "           1       0.61      0.63      0.62       200\n",
      "           2       0.91      0.83      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "57\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.75\n",
      "Accuracy =  0.75\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152           48         0\n",
      "Prediabetes      53          122        25\n",
      "diabetes          3           21       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75       200\n",
      "           1       0.64      0.61      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7416666666666667\n",
      "Accuracy =  0.7416666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           49         1\n",
      "Prediabetes      53          121        26\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.63      0.60      0.62       200\n",
      "           2       0.87      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "59\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7316666666666667\n",
      "Accuracy =  0.7316666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          147           52         1\n",
      "Prediabetes      55          127        18\n",
      "diabetes          4           31       165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72       200\n",
      "           1       0.60      0.64      0.62       200\n",
      "           2       0.90      0.82      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.73       600\n",
      "weighted avg       0.74      0.73      0.73       600\n",
      "\n",
      "60\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.75\n",
      "Accuracy =  0.75\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          154           46         0\n",
      "Prediabetes      54          121        25\n",
      "diabetes          4           21       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       200\n",
      "           1       0.64      0.60      0.62       200\n",
      "           2       0.88      0.88      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "61\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7383333333333333\n",
      "Accuracy =  0.7383333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           50         1\n",
      "Prediabetes      54          118        28\n",
      "diabetes          4           20       176\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       200\n",
      "           1       0.63      0.59      0.61       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "62\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7366666666666667\n",
      "Accuracy =  0.7366666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      52          127        21\n",
      "diabetes          4           31       165\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       200\n",
      "           1       0.61      0.64      0.62       200\n",
      "           2       0.89      0.82      0.85       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "63\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          153           47         0\n",
      "Prediabetes      53          122        25\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.77      0.75       200\n",
      "           1       0.64      0.61      0.62       200\n",
      "           2       0.87      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "64\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7366666666666667\n",
      "Accuracy =  0.7366666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           50         1\n",
      "Prediabetes      54          119        27\n",
      "diabetes          4           22       174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       200\n",
      "           1       0.62      0.59      0.61       200\n",
      "           2       0.86      0.87      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.73      0.74      0.74       600\n",
      "weighted avg       0.73      0.74      0.74       600\n",
      "\n",
      "65\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      53          128        19\n",
      "diabetes          3           26       171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       200\n",
      "           1       0.63      0.64      0.63       200\n",
      "           2       0.90      0.85      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "66\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.745\n",
      "Accuracy =  0.745\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           49         0\n",
      "Prediabetes      51          125        24\n",
      "diabetes          3           26       171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75       200\n",
      "           1       0.62      0.62      0.62       200\n",
      "           2       0.88      0.85      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.75      0.74      0.75       600\n",
      "weighted avg       0.75      0.74      0.75       600\n",
      "\n",
      "67\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.6033333333333334\n",
      "Accuracy =  0.6033333333333334\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          154           21        25\n",
      "Prediabetes      48           45       107\n",
      "diabetes          3           34       163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       200\n",
      "           1       0.45      0.23      0.30       200\n",
      "           2       0.55      0.81      0.66       200\n",
      "\n",
      "    accuracy                           0.60       600\n",
      "   macro avg       0.58      0.60      0.57       600\n",
      "weighted avg       0.58      0.60      0.57       600\n",
      "\n",
      "68\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      53          131        16\n",
      "diabetes          3           28       169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.74       200\n",
      "           1       0.62      0.66      0.64       200\n",
      "           2       0.91      0.84      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "69\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7466666666666667\n",
      "Accuracy =  0.7466666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      50          126        24\n",
      "diabetes          3           24       173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       200\n",
      "           1       0.63      0.63      0.63       200\n",
      "           2       0.88      0.86      0.87       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "70\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7516666666666667\n",
      "Accuracy =  0.7516666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152           47         1\n",
      "Prediabetes      49          124        27\n",
      "diabetes          3           22       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75       200\n",
      "           1       0.64      0.62      0.63       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "71\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      55          130        15\n",
      "diabetes          3           28       169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.62      0.65      0.64       200\n",
      "           2       0.92      0.84      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "72\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.74\n",
      "Accuracy =  0.74\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      54          123        23\n",
      "diabetes          3           26       171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.62      0.61      0.62       200\n",
      "           2       0.88      0.85      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "73\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7433333333333333\n",
      "Accuracy =  0.7433333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           47         3\n",
      "Prediabetes      51          119        30\n",
      "diabetes          3           20       177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.74       200\n",
      "           1       0.64      0.59      0.62       200\n",
      "           2       0.84      0.89      0.86       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "74\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.735\n",
      "Accuracy =  0.735\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      55          125        20\n",
      "diabetes          3           30       167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       200\n",
      "           1       0.61      0.62      0.62       200\n",
      "           2       0.89      0.83      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.74       600\n",
      "weighted avg       0.74      0.73      0.74       600\n",
      "\n",
      "75\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7416666666666667\n",
      "Accuracy =  0.7416666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           49         0\n",
      "Prediabetes      53          123        24\n",
      "diabetes          3           26       171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74       200\n",
      "           1       0.62      0.61      0.62       200\n",
      "           2       0.88      0.85      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "76\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.58\n",
      "Accuracy =  0.58\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152            7        41\n",
      "Prediabetes      53           18       129\n",
      "diabetes          3           19       178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75       200\n",
      "           1       0.41      0.09      0.15       200\n",
      "           2       0.51      0.89      0.65       200\n",
      "\n",
      "    accuracy                           0.58       600\n",
      "   macro avg       0.55      0.58      0.51       600\n",
      "weighted avg       0.55      0.58      0.51       600\n",
      "\n",
      "77\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7366666666666667\n",
      "Accuracy =  0.7366666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          147           53         0\n",
      "Prediabetes      54          128        18\n",
      "diabetes          3           30       167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.73       200\n",
      "           1       0.61      0.64      0.62       200\n",
      "           2       0.90      0.83      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "78\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.74\n",
      "Accuracy =  0.74\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      52          123        25\n",
      "diabetes          3           25       172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.74       200\n",
      "           1       0.62      0.61      0.62       200\n",
      "           2       0.87      0.86      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "79\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.755\n",
      "Accuracy =  0.755\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          154           45         1\n",
      "Prediabetes      49          124        27\n",
      "diabetes          3           22       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       200\n",
      "           1       0.65      0.62      0.63       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.76       600\n",
      "   macro avg       0.75      0.76      0.75       600\n",
      "weighted avg       0.75      0.76      0.75       600\n",
      "\n",
      "80\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      53          128        19\n",
      "diabetes          3           26       171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.75      0.74       200\n",
      "           1       0.63      0.64      0.63       200\n",
      "           2       0.90      0.85      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "81\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.745\n",
      "Accuracy =  0.745\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           49         0\n",
      "Prediabetes      51          125        24\n",
      "diabetes          3           26       171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75       200\n",
      "           1       0.62      0.62      0.62       200\n",
      "           2       0.88      0.85      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.75      0.74      0.75       600\n",
      "weighted avg       0.75      0.74      0.75       600\n",
      "\n",
      "82\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.6033333333333334\n",
      "Accuracy =  0.6033333333333334\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          154           21        25\n",
      "Prediabetes      48           45       107\n",
      "diabetes          3           34       163\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       200\n",
      "           1       0.45      0.23      0.30       200\n",
      "           2       0.55      0.81      0.66       200\n",
      "\n",
      "    accuracy                           0.60       600\n",
      "   macro avg       0.58      0.60      0.57       600\n",
      "weighted avg       0.58      0.60      0.57       600\n",
      "\n",
      "83\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      53          131        16\n",
      "diabetes          3           28       169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.74       200\n",
      "           1       0.62      0.66      0.64       200\n",
      "           2       0.91      0.84      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "84\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7466666666666667\n",
      "Accuracy =  0.7466666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      50          126        24\n",
      "diabetes          3           24       173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       200\n",
      "           1       0.63      0.63      0.63       200\n",
      "           2       0.88      0.86      0.87       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "85\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7516666666666667\n",
      "Accuracy =  0.7516666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152           47         1\n",
      "Prediabetes      49          124        27\n",
      "diabetes          3           22       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75       200\n",
      "           1       0.64      0.62      0.63       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "86\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7483333333333333\n",
      "Accuracy =  0.7483333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      55          130        15\n",
      "diabetes          3           28       169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.62      0.65      0.64       200\n",
      "           2       0.92      0.84      0.88       200\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.75      0.75      0.75       600\n",
      "weighted avg       0.75      0.75      0.75       600\n",
      "\n",
      "87\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.74\n",
      "Accuracy =  0.74\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           50         0\n",
      "Prediabetes      54          123        23\n",
      "diabetes          3           26       171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74       200\n",
      "           1       0.62      0.61      0.62       200\n",
      "           2       0.88      0.85      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "88\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7433333333333333\n",
      "Accuracy =  0.7433333333333333\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          150           47         3\n",
      "Prediabetes      51          119        30\n",
      "diabetes          3           20       177\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.74       200\n",
      "           1       0.64      0.59      0.62       200\n",
      "           2       0.84      0.89      0.86       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "89\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.735\n",
      "Accuracy =  0.735\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      55          125        20\n",
      "diabetes          3           30       167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       200\n",
      "           1       0.61      0.62      0.62       200\n",
      "           2       0.89      0.83      0.86       200\n",
      "\n",
      "    accuracy                           0.73       600\n",
      "   macro avg       0.74      0.73      0.74       600\n",
      "weighted avg       0.74      0.73      0.74       600\n",
      "\n",
      "90\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7416666666666667\n",
      "Accuracy =  0.7416666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Normal  Prediabetes  diabetes\n",
      "Normal          151           49         0\n",
      "Prediabetes      53          123        24\n",
      "diabetes          3           26       171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74       200\n",
      "           1       0.62      0.61      0.62       200\n",
      "           2       0.88      0.85      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "91\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.58\n",
      "Accuracy =  0.58\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          152            7        41\n",
      "Prediabetes      53           18       129\n",
      "diabetes          3           19       178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75       200\n",
      "           1       0.41      0.09      0.15       200\n",
      "           2       0.51      0.89      0.65       200\n",
      "\n",
      "    accuracy                           0.58       600\n",
      "   macro avg       0.55      0.58      0.51       600\n",
      "weighted avg       0.55      0.58      0.51       600\n",
      "\n",
      "92\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.7366666666666667\n",
      "Accuracy =  0.7366666666666667\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          147           53         0\n",
      "Prediabetes      54          128        18\n",
      "diabetes          3           30       167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.73       200\n",
      "           1       0.61      0.64      0.62       200\n",
      "           2       0.90      0.83      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "93\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.74\n",
      "Accuracy =  0.74\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          149           51         0\n",
      "Prediabetes      52          123        25\n",
      "diabetes          3           25       172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.74       200\n",
      "           1       0.62      0.61      0.62       200\n",
      "           2       0.87      0.86      0.87       200\n",
      "\n",
      "    accuracy                           0.74       600\n",
      "   macro avg       0.74      0.74      0.74       600\n",
      "weighted avg       0.74      0.74      0.74       600\n",
      "\n",
      "94\n",
      "<class 'sklearn.pipeline.Pipeline'> accuracy \t 0.755\n",
      "Accuracy =  0.755\n",
      "             Normal  Prediabetes  diabetes\n",
      "Normal          154           45         1\n",
      "Prediabetes      49          124        27\n",
      "diabetes          3           22       175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       200\n",
      "           1       0.65      0.62      0.63       200\n",
      "           2       0.86      0.88      0.87       200\n",
      "\n",
      "    accuracy                           0.76       600\n",
      "   macro avg       0.75      0.76      0.75       600\n",
      "weighted avg       0.75      0.76      0.75       600\n",
      "\n",
      "95\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"stackedmodels = sclf.clfs_\\nfor i in range(len(stackedmodels)):\\n    ypred = stackedmodels[i].predict(xtest)\\n    print(type(sclf.clfs_[i]), \\\"accuracy \\\\t\\\", m.accuracy_score(ytest, ypred))\\n    print(\\\"Accuracy = \\\", m.accuracy_score(ytest, ypred))\\n\\n    confmatrx = pd.DataFrame(\\n        m.confusion_matrix(ytest, ypred),\\n        columns=[\\\"Normal\\\", \\\"Prediabetes\\\", \\\"diabetes\\\"],\\n        index=[\\\"Normal\\\", \\\"Prediabetes\\\", \\\"diabetes\\\"],\\n    )\\n    print(confmatrx.head())\\n\\n    print(m.classification_report(ytest, ypred))\\n\\n    print(i)\";\n",
       "                var nbb_formatted_code = \"stackedmodels = sclf.clfs_\\nfor i in range(len(stackedmodels)):\\n    ypred = stackedmodels[i].predict(xtest)\\n    print(type(sclf.clfs_[i]), \\\"accuracy \\\\t\\\", m.accuracy_score(ytest, ypred))\\n    print(\\\"Accuracy = \\\", m.accuracy_score(ytest, ypred))\\n\\n    confmatrx = pd.DataFrame(\\n        m.confusion_matrix(ytest, ypred),\\n        columns=[\\\"Normal\\\", \\\"Prediabetes\\\", \\\"diabetes\\\"],\\n        index=[\\\"Normal\\\", \\\"Prediabetes\\\", \\\"diabetes\\\"],\\n    )\\n    print(confmatrx.head())\\n\\n    print(m.classification_report(ytest, ypred))\\n\\n    print(i)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stackedmodels = sclf.clfs_\n",
    "for i in range(len(stackedmodels)):\n",
    "    ypred = stackedmodels[i].predict(xtest)\n",
    "    print(type(sclf.clfs_[i]), \"accuracy \\t\", m.accuracy_score(ytest, ypred))\n",
    "    print(\"Accuracy = \", m.accuracy_score(ytest, ypred))\n",
    "\n",
    "    confmatrx = pd.DataFrame(\n",
    "        m.confusion_matrix(ytest, ypred),\n",
    "        columns=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "        index=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "    )\n",
    "    print(confmatrx.head())\n",
    "\n",
    "    print(m.classification_report(ytest, ypred))\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generate Votting Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"votingclf = VotingClassifier(estimators=estimators, voting=\\\"hard\\\")\";\n",
       "                var nbb_formatted_code = \"votingclf = VotingClassifier(estimators=estimators, voting=\\\"hard\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "votingclf = VotingClassifier(estimators=estimators, voting=\"hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.0s finished\n"
     ]
    }
   ],
   "source": [
    "votingclf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ypred = votingclf.predict(xtest.astype(float))\n",
    "\n",
    "print(\"accuracy \\t\", m.accuracy_score(ytest, ypred))\n",
    "confmatrx = pd.DataFrame(\n",
    "    m.confusion_matrix(ytest, ypred),\n",
    "    columns=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "    index=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    ")\n",
    "confmatrx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stackedmodels = votingclf.estimators_\n",
    "for i in range(len(stackedmodels)):\n",
    "    ypred = stackedmodels[i].predict(xtest)\n",
    "    print(type(sclf.clfs_[i]), \"accuracy \\t\", m.accuracy_score(ytest, ypred))\n",
    "    print(\"Accuracy = \", m.accuracy_score(ytest, ypred))\n",
    "\n",
    "    confmatrx = pd.DataFrame(\n",
    "        m.confusion_matrix(ytest, ypred),\n",
    "        columns=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "        index=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "    )\n",
    "    print(confmatrx.head())\n",
    "\n",
    "    print(m.classification_report(ytest, ypred))\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6  Generate Votting Classifer soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scv_model_cpu_withProb = SVC(\n",
    "  C=100,\n",
    "    cache_size=100,\n",
    "    class_weight={},\n",
    "    coef0=0.0,\n",
    "    decision_function_shape=\"ovo\",\n",
    "    degree=3,\n",
    "    gamma=0.1,\n",
    "#     gpu_id=0,\n",
    "    kernel=\"linear\",\n",
    "    max_iter=-1,\n",
    "#     max_mem_size=-1,\n",
    "#     n_jobs=-1,\n",
    "    probability=True,\n",
    "    random_state=randomseed,\n",
    "    shrinking=False,\n",
    "    tol=0.001,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingclf2 = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"rf_model\", rf_model),\n",
    "        (\"xgb_model\", xgb_model),\n",
    "        (\"scv_model\", scv_model_cpu_withProb),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingclf2.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = votingclf2.predict(xtest.astype(float))\n",
    "\n",
    "print(\"accuracy \\t\", m.accuracy_score(ytest, ypred))\n",
    "confmatrx = pd.DataFrame(\n",
    "    m.confusion_matrix(ytest, ypred),\n",
    "    columns=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "    index=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    ")\n",
    "confmatrx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedmodels = votingclf2.estimators_\n",
    "for i in range(len(stackedmodels)):\n",
    "    ypred = stackedmodels[i].predict(xtest)\n",
    "    print(type(sclf.clfs_[i]), \"accuracy \\t\", m.accuracy_score(ytest, ypred))\n",
    "    print(\"Accuracy = \", m.accuracy_score(ytest, ypred))\n",
    "\n",
    "    confmatrx = pd.DataFrame(\n",
    "        m.confusion_matrix(ytest, ypred),\n",
    "        columns=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "        index=[\"Normal\", \"Prediabetes\", \"diabetes\"],\n",
    "    )\n",
    "    print(confmatrx.head())\n",
    "\n",
    "    print(m.classification_report(ytest, ypred))\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
