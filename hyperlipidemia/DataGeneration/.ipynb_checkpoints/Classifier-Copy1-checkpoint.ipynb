{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics as m\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE  # doctest: +NORMALIZE_WHITESPACE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "randomseed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37065, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FIELD_1</th>\n",
       "      <th>FIELD_2</th>\n",
       "      <th>L190500</th>\n",
       "      <th>L190300</th>\n",
       "      <th>L101300</th>\n",
       "      <th>L100700</th>\n",
       "      <th>S000300</th>\n",
       "      <th>L101700</th>\n",
       "      <th>L100800</th>\n",
       "      <th>...</th>\n",
       "      <th>L103100</th>\n",
       "      <th>FIELD_33</th>\n",
       "      <th>FIELD_38</th>\n",
       "      <th>FIELD_40</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>L102900</th>\n",
       "      <th>L103200</th>\n",
       "      <th>L103000</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>674978</td>\n",
       "      <td>20160716</td>\n",
       "      <td>41.2</td>\n",
       "      <td>4.62</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2796877</td>\n",
       "      <td>20151211</td>\n",
       "      <td>41.1</td>\n",
       "      <td>4.54</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>23.9</td>\n",
       "      <td>16.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>430189</td>\n",
       "      <td>20160713</td>\n",
       "      <td>38.3</td>\n",
       "      <td>4.31</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>23.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>988885</td>\n",
       "      <td>20150429</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>24.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>342048</td>\n",
       "      <td>20161026</td>\n",
       "      <td>39.3</td>\n",
       "      <td>4.10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>17.7</td>\n",
       "      <td>28.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  FIELD_1   FIELD_2  L190500  L190300  L101300  L100700  S000300  \\\n",
       "0           0   674978  20160716     41.2     4.62     11.0      4.1     22.2   \n",
       "1           1  2796877  20151211     41.1     4.54     15.0      5.9     23.9   \n",
       "2           2   430189  20160713     38.3     4.31     12.0      3.3     23.1   \n",
       "3           3   988885  20150429     44.0     4.93     10.0      5.4     24.4   \n",
       "4           4   342048  20161026     39.3     4.10     14.0      3.9     17.7   \n",
       "\n",
       "   L101700  L100800  ...  L103100  FIELD_33  FIELD_38  FIELD_40  SEX   AGE  \\\n",
       "0     20.0     93.0  ...     62.0       1.0       1.0       0.0  1.0  39.0   \n",
       "1     16.0     92.0  ...     38.0       1.0       1.0       1.0  0.0  40.0   \n",
       "2     12.0     84.0  ...     57.0       1.0       1.0       0.0  1.0  34.0   \n",
       "3     21.0     93.0  ...     34.0       2.0       2.0       2.0  0.0  48.0   \n",
       "4     28.0     91.0  ...     64.0       3.0       0.0       1.0  1.0  32.0   \n",
       "\n",
       "   L102900  L103200  L103000  CLASS  \n",
       "0    169.0     88.0     96.0      0  \n",
       "1    204.0    149.0     85.0      1  \n",
       "2    166.0     96.0     64.0      0  \n",
       "3    181.0    101.0    229.0      1  \n",
       "4    212.0    129.0     94.0      1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv(\"NextYearData_actualData_train.txt\")\n",
    "data2 = pd.read_csv(\"ThisYearData_actualData_train.txt\")\n",
    "data3 = pd.read_csv(\"ThisYearData_actualData_test.txt\")\n",
    "\n",
    "data4 = pd.read_csv(\"NextYearData_actualData_test.txt\")\n",
    "\n",
    "data5 = pd.read_csv(\"Predicted_NextYearData.txt\")\n",
    "\n",
    "data = pd.concat([data1, data2, data3])\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "data.iloc[:,:-1]=scaler.fit_transform(data.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'FIELD_1', 'FIELD_2', 'L190500', 'L190300', 'L101300',\n",
       "       'L100700', 'S000300', 'L101700', 'L100800', 'L103300', 'L103100',\n",
       "       'FIELD_33', 'FIELD_38', 'FIELD_40', 'SEX', 'AGE', 'L102900', 'L103200',\n",
       "       'L103000', 'CLASS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempcols=['L190500', 'L190300', 'L101300','L100700', 'S000300', 'L101700', 'L100800','L103300', 'L103100',\n",
    "          'FIELD_33', 'FIELD_38', 'FIELD_40','SEX', 'AGE', #'L102900', 'L103200','L103000',          \n",
    "        'CLASS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['L190500', 'L190300', 'L101300','L100700', 'S000300', 'L101700', 'L100800','L103300', 'L103100',\n",
    "          'FIELD_33', 'FIELD_38', 'FIELD_40','SEX', 'AGE',  'CLASS']]\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    random_state=randomseed,\n",
    "    n_estimators=1000,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=10,\n",
    "    max_features=8,\n",
    ")\n",
    "rf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = rf.predict(xtest)\n",
    "\n",
    "score = rf.score(xtest, ytest)\n",
    "print(score)\n",
    "\n",
    "feat_importances = pd.Series(rf.feature_importances_, index=xtest.columns)\n",
    "feat_importances.nlargest(16).plot(kind=\"barh\")\n",
    "plt.show()\n",
    "\n",
    "print(feat_importances.nlargest(16).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training set: {:.3f}\".format(rf.score(xtrain, ytrain)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(rf.score(xtest, ytest)))\n",
    "\n",
    "confmatrx = pd.DataFrame(m.confusion_matrix(ytest, ypred))\n",
    "confmatrx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn import metrics as m\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=randomseed)\n",
    "\n",
    "# xgb_model.fit(xtrain, ytrain)\n",
    "\n",
    "# y_pred = xgb_model.predict((xtest))\n",
    "\n",
    "# print(\"accuracy \\t\", m.accuracy_score(ytest, y_pred))\n",
    "# confmatrx = pd.DataFrame(m.confusion_matrix(ytest, y_pred))\n",
    "# confmatrx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37065, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data4=data4[['L190500','L190300','L101300','L100700','S000300','L101700','L100800','L103300','L103100',\n",
    "#              'FIELD_33','FIELD_38','FIELD_40','SEX','AGE','CLASS']]\n",
    "\n",
    "data4=data4[['L190500', 'L190300', 'L101300','L100700', 'S000300', 'L101700', 'L100800','L103300', 'L103100',\n",
    "          'FIELD_33', 'FIELD_38', 'FIELD_40','SEX', 'AGE',  'CLASS']]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6541,14) (20,) (6541,14) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3911ba3f6e68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    387\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6541,14) (20,) (6541,14) "
     ]
    }
   ],
   "source": [
    "scaler.transform(data4.iloc[:, :-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4=data4[['L190500','L190300','L101300','L100700','S000300','L101700','L100800','L103300','L103100',\n",
    "             'FIELD_33','FIELD_38','FIELD_40','SEX','AGE','CLASS']]\n",
    "\n",
    "data4.iloc[:, :-1]=scaler.transform(data4.iloc[:, :-1])\n",
    "pred4 = rf.predict((data4.iloc[:, :-1]))\n",
    "\n",
    "print(m.accuracy_score(data4.iloc[:, -1], pred4))\n",
    "\n",
    "confmatrx = pd.DataFrame(m.confusion_matrix(data4.iloc[:, -1], pred4))\n",
    "confmatrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(data4.iloc[:, -1], pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# =====================================\n",
    "# =====================================\n",
    "# =====================================\n",
    "\n",
    "data5=data5[['P_L190500','P_L190300','P_L101300','P_L100700','P_S000300','P_L101700','P_L100800','P_L103300','P_L103100','P_FIELD_33','P_FIELD_38',\n",
    "             'P_FIELD_40','P_SEX','P_AGE','CLASS']]\n",
    "\n",
    "\n",
    "data5.iloc[:, :-1]=scaler.transform(data5.iloc[:, :-1])\n",
    "\n",
    "pred5 = rf.predict((data5.iloc[:, :-1]))\n",
    "\n",
    "\n",
    "\n",
    "print(m.accuracy_score(data5.iloc[:, -1], pred5))\n",
    "\n",
    "confmatrx = pd.DataFrame(m.confusion_matrix(data5.iloc[:, -1], pred5))\n",
    "confmatrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(data5.iloc[:, -1], pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('HyperlipidemiaModelClassifierModelForNextYear_rf_model', 'wb') as f:\n",
    "    pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HyperlipidemiaClassifierModelForNextYear_scaler_SMOTE', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('_train.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4.to_csv('_test.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data5.to_csv('_PredictedNextYearTest.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
