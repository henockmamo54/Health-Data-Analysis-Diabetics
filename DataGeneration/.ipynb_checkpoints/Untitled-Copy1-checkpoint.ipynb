{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics as m\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE # doctest: +NORMALIZE_WHITESPACE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "randomseed=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('../sep19SexAndAgeAddedFINAL DATASET_ver2.txt',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data set \n",
    "data=data[data.FIELD_16!=1] # exclude people who are diagnosed for (diabetes) \n",
    "data=data[data.FIELD_23!=1] # exclude people who are on medication for diabetes\n",
    "\n",
    "data=data[data.FIELD_15!=1] # exclude people who are diagnosed for (high blood pressure)\n",
    "data=data[data.FIELD_22!=1] # exclude people who are on medication for high blood pressure\n",
    "\n",
    "data=data[data.FIELD_17!=1] # exclude people who are diagnosed for hyperlipidemia\n",
    "data=data[data.FIELD_24!=1] # exclude people who are on medication for hyperlipidemia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Class to the dataset \n",
    "conditions = [\n",
    "    (data.L100800 < 100)  ,\n",
    "    (data.L100800 >= 100) & (data.L100800 < 126),\n",
    "    (data.L100800 >= 126)]\n",
    "choices = [0,1,2]\n",
    "data['CLASS'] = np.select(conditions, choices, default=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142716, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata=data.copy()[['L104600','L103000','S000300','L101700','L100700','FIELD_33','FIELD_38','FIELD_40','S000501', 'S000502',\n",
    "                    'FIELD_31','SEX','AGE','CLASS']].dropna()\n",
    "mydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({1: 104030, 0: 104030, 2: 104030})\n",
      "104030 104030 104030\n",
      "(312090, 13) (312090,)\n"
     ]
    }
   ],
   "source": [
    "x=mydata[['L104600','L103000','S000300','L101700','L100700','FIELD_33','FIELD_38','FIELD_40','S000501', 'S000502',\n",
    "                    'FIELD_31','SEX','AGE']]\n",
    "y=mydata[['CLASS']]\n",
    "\n",
    "\n",
    "sm = SMOTE(random_state=randomseed)\n",
    "X_res, y_res = sm.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))\n",
    "print(y_res[y_res==0].shape[0], y_res[y_res==1].shape[0], y_res[y_res==2].shape[0])\n",
    "\n",
    "print(X_res.shape,y_res.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res2=y_res.copy()\n",
    "from keras.utils import to_categorical\n",
    "y_res2 = to_categorical(y_res)  \n",
    "\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X_res,y_res2,random_state=randomseed, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2451 36235 104030\n"
     ]
    }
   ],
   "source": [
    "diabetic = mydata[mydata.CLASS==2]\n",
    "prediabetic = mydata[mydata.CLASS==1]\n",
    "normal = mydata[mydata.CLASS==0]\n",
    "\n",
    "print(diabetic.shape[0],prediabetic.shape[0],normal.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetic_test = diabetic.sample(500,random_state=randomseed)\n",
    "prediabetic_test = prediabetic.sample(500,random_state=randomseed)\n",
    "normal_test = normal.sample(500,random_state=randomseed)\n",
    "test=pd.concat([diabetic_test,prediabetic_test,normal_test])\n",
    "\n",
    "diabetic_train = diabetic.drop(diabetic_test.index)\n",
    "prediabetic_train = prediabetic.drop(prediabetic_test.index).sample(diabetic_train.shape[0],random_state=randomseed)\n",
    "normal_train = normal.drop(normal_test.index).sample(diabetic_train.shape[0],random_state=randomseed)\n",
    "train=pd.concat([diabetic_train,prediabetic_train,normal_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain=train.iloc[:,:-1]\n",
    "# ytrain=train.iloc[:,-1]\n",
    "# xtest=test.iloc[:,:-1]\n",
    "# ytest=test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=7, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf=RandomForestClassifier(random_state=randomseed,n_estimators=100,max_depth=10 ) \n",
    "rf.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred= rf.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8014354833541607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80     20709\n",
      "           1       0.76      0.70      0.73     20827\n",
      "           2       0.94      0.90      0.92     20882\n",
      "\n",
      "   micro avg       0.84      0.80      0.82     62418\n",
      "   macro avg       0.83      0.80      0.82     62418\n",
      "weighted avg       0.83      0.80      0.82     62418\n",
      " samples avg       0.80      0.80      0.80     62418\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['L104600', 'FIELD_33', 'SEX', 'L101700', 'L103000', 'FIELD_38', 'AGE',\n",
      "       'S000300', 'FIELD_40', 'S000501', 'FIELD_31', 'S000502', 'L100700'],\n",
      "      dtype='object')\n",
      "Accuracy on training set: 0.811\n",
      "Accuracy on test set: 0.801\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-adc8dba0b6e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mconfmatrx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mypred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mconfmatrx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "score=rf.score(xtest,ytest)\n",
    "print(score)\n",
    "\n",
    "print(classification_report(ytest,ypred))\n",
    "\n",
    "feat_importances = pd.Series(rf.feature_importances_, index=test.iloc[:,:-1].columns)\n",
    "feat_importances.nlargest(16).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "print(feat_importances.nlargest(16).index)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(rf.score(xtrain,ytrain)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(rf.score(xtest,ytest ))) \n",
    "\n",
    "\n",
    "confmatrx=pd.DataFrame(confusion_matrix(ytest,ypred))\n",
    "confmatrx.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import multi_gpu_model \n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(input_dim=13,units=128,activation='relu', kernel_initializer='uniform'))\n",
    "model.add(Dense(units=128, activation='relu', kernel_initializer='uniform')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))  \n",
    "model.add(Dense(units=64, activation='relu', kernel_initializer='uniform'))\n",
    "model.add(Dense(units=32, activation='relu', kernel_initializer='uniform')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))  \n",
    "model.add(Dense(units=16, activation='relu', kernel_initializer='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))  \n",
    "model.add(Dense(units=8, activation='relu', kernel_initializer='uniform'))\n",
    "model.add(Dense(units=3, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1015 01:38:44.004148 14512 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 199737 samples, validate on 49935 samples\n",
      "Epoch 1/100\n",
      "199737/199737 [==============================] - 8s 39us/step - loss: 0.6093 - acc: 0.7139 - val_loss: 0.6907 - val_acc: 0.6722\n",
      "Epoch 2/100\n",
      "199737/199737 [==============================] - 7s 35us/step - loss: 0.5721 - acc: 0.7336 - val_loss: 0.5430 - val_acc: 0.7458\n",
      "Epoch 3/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5672 - acc: 0.7362 - val_loss: 0.5483 - val_acc: 0.7400\n",
      "Epoch 4/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5636 - acc: 0.7391 - val_loss: 0.5267 - val_acc: 0.7546\n",
      "Epoch 5/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5592 - acc: 0.7401 - val_loss: 0.5508 - val_acc: 0.7387\n",
      "Epoch 6/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5550 - acc: 0.7427 - val_loss: 0.5697 - val_acc: 0.7319\n",
      "Epoch 7/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5538 - acc: 0.7416 - val_loss: 0.5294 - val_acc: 0.7508\n",
      "Epoch 8/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5507 - acc: 0.7433 - val_loss: 0.5365 - val_acc: 0.7475\n",
      "Epoch 9/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5505 - acc: 0.7442 - val_loss: 0.6402 - val_acc: 0.7002\n",
      "Epoch 10/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5498 - acc: 0.7435 - val_loss: 0.5604 - val_acc: 0.7377\n",
      "Epoch 11/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5486 - acc: 0.7445 - val_loss: 0.5333 - val_acc: 0.7514\n",
      "Epoch 12/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5477 - acc: 0.7444 - val_loss: 0.5468 - val_acc: 0.7432\n",
      "Epoch 13/100\n",
      "199737/199737 [==============================] - 7s 33us/step - loss: 0.5471 - acc: 0.7459 - val_loss: 0.5763 - val_acc: 0.7248\n",
      "Epoch 14/100\n",
      "199737/199737 [==============================] - 7s 33us/step - loss: 0.5463 - acc: 0.7463 - val_loss: 0.5300 - val_acc: 0.7508\n",
      "Epoch 15/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5460 - acc: 0.7446 - val_loss: 0.5331 - val_acc: 0.7511\n",
      "Epoch 16/100\n",
      "199737/199737 [==============================] - 7s 35us/step - loss: 0.5448 - acc: 0.7473 - val_loss: 0.5497 - val_acc: 0.7432\n",
      "Epoch 17/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5440 - acc: 0.7458 - val_loss: 0.5323 - val_acc: 0.7480\n",
      "Epoch 18/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5433 - acc: 0.7464 - val_loss: 0.5366 - val_acc: 0.7478\n",
      "Epoch 19/100\n",
      "199737/199737 [==============================] - 7s 33us/step - loss: 0.5425 - acc: 0.7477 - val_loss: 0.6651 - val_acc: 0.6853\n",
      "Epoch 20/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5419 - acc: 0.7478 - val_loss: 0.5554 - val_acc: 0.7331\n",
      "Epoch 21/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5401 - acc: 0.7486 - val_loss: 0.5296 - val_acc: 0.7527\n",
      "Epoch 22/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5395 - acc: 0.7484 - val_loss: 0.5190 - val_acc: 0.7573\n",
      "Epoch 23/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5394 - acc: 0.7491 - val_loss: 0.5191 - val_acc: 0.7563\n",
      "Epoch 24/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5382 - acc: 0.7496 - val_loss: 0.5262 - val_acc: 0.7552\n",
      "Epoch 25/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5385 - acc: 0.7496 - val_loss: 0.5746 - val_acc: 0.7211\n",
      "Epoch 26/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5367 - acc: 0.7503 - val_loss: 0.5199 - val_acc: 0.7564\n",
      "Epoch 27/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5358 - acc: 0.7496 - val_loss: 0.5264 - val_acc: 0.7540\n",
      "Epoch 28/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5357 - acc: 0.7508 - val_loss: 0.5199 - val_acc: 0.7541\n",
      "Epoch 29/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5356 - acc: 0.7510 - val_loss: 0.5170 - val_acc: 0.7571\n",
      "Epoch 30/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5354 - acc: 0.7510 - val_loss: 0.5165 - val_acc: 0.7575\n",
      "Epoch 31/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5337 - acc: 0.7518 - val_loss: 0.5141 - val_acc: 0.7593\n",
      "Epoch 32/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5344 - acc: 0.7507 - val_loss: 0.5129 - val_acc: 0.7588\n",
      "Epoch 33/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5332 - acc: 0.7526 - val_loss: 0.5515 - val_acc: 0.7342\n",
      "Epoch 34/100\n",
      "199737/199737 [==============================] - 7s 33us/step - loss: 0.5325 - acc: 0.7533 - val_loss: 0.5419 - val_acc: 0.7418\n",
      "Epoch 35/100\n",
      "199737/199737 [==============================] - 7s 33us/step - loss: 0.5332 - acc: 0.7512 - val_loss: 0.5176 - val_acc: 0.7562\n",
      "Epoch 36/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5336 - acc: 0.7515 - val_loss: 0.5239 - val_acc: 0.7518\n",
      "Epoch 37/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5334 - acc: 0.7520 - val_loss: 0.5306 - val_acc: 0.7479\n",
      "Epoch 38/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5333 - acc: 0.7521 - val_loss: 0.5970 - val_acc: 0.7162\n",
      "Epoch 39/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5321 - acc: 0.7520 - val_loss: 0.5128 - val_acc: 0.7608\n",
      "Epoch 40/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5325 - acc: 0.7528 - val_loss: 0.5112 - val_acc: 0.7596\n",
      "Epoch 41/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5315 - acc: 0.7531 - val_loss: 0.5138 - val_acc: 0.7588\n",
      "Epoch 42/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5318 - acc: 0.7523 - val_loss: 0.5260 - val_acc: 0.7552\n",
      "Epoch 43/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5305 - acc: 0.7528 - val_loss: 0.5127 - val_acc: 0.7578\n",
      "Epoch 44/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5308 - acc: 0.7532 - val_loss: 0.5140 - val_acc: 0.7596\n",
      "Epoch 45/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5302 - acc: 0.7534 - val_loss: 0.5143 - val_acc: 0.7577\n",
      "Epoch 46/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5307 - acc: 0.7531 - val_loss: 0.5238 - val_acc: 0.7530\n",
      "Epoch 47/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5289 - acc: 0.7548 - val_loss: 0.5143 - val_acc: 0.7601\n",
      "Epoch 48/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5303 - acc: 0.7536 - val_loss: 0.5326 - val_acc: 0.7477\n",
      "Epoch 49/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5287 - acc: 0.7545 - val_loss: 0.5162 - val_acc: 0.7583\n",
      "Epoch 50/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5297 - acc: 0.7545 - val_loss: 0.5156 - val_acc: 0.7590\n",
      "Epoch 51/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5285 - acc: 0.7542 - val_loss: 0.5107 - val_acc: 0.7621\n",
      "Epoch 52/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5300 - acc: 0.7536 - val_loss: 0.5358 - val_acc: 0.7503\n",
      "Epoch 53/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5284 - acc: 0.7545 - val_loss: 0.5087 - val_acc: 0.7631\n",
      "Epoch 54/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5280 - acc: 0.7537 - val_loss: 0.5268 - val_acc: 0.7503\n",
      "Epoch 55/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5279 - acc: 0.7552 - val_loss: 0.5278 - val_acc: 0.7553\n",
      "Epoch 56/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5267 - acc: 0.7551 - val_loss: 0.5176 - val_acc: 0.7589\n",
      "Epoch 57/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5271 - acc: 0.7549 - val_loss: 0.5204 - val_acc: 0.7561\n",
      "Epoch 58/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5282 - acc: 0.7542 - val_loss: 0.5108 - val_acc: 0.7619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5265 - acc: 0.7559 - val_loss: 0.5059 - val_acc: 0.7642\n",
      "Epoch 60/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5268 - acc: 0.7551 - val_loss: 0.5247 - val_acc: 0.7557\n",
      "Epoch 61/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5262 - acc: 0.7552 - val_loss: 0.5204 - val_acc: 0.7562\n",
      "Epoch 62/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5256 - acc: 0.7557 - val_loss: 0.5104 - val_acc: 0.7599\n",
      "Epoch 63/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5267 - acc: 0.7545 - val_loss: 0.5050 - val_acc: 0.7646\n",
      "Epoch 64/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5264 - acc: 0.7559 - val_loss: 0.5103 - val_acc: 0.7614\n",
      "Epoch 65/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5261 - acc: 0.7553 - val_loss: 0.5116 - val_acc: 0.7605\n",
      "Epoch 66/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5255 - acc: 0.7552 - val_loss: 0.5162 - val_acc: 0.7578\n",
      "Epoch 67/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5244 - acc: 0.7560 - val_loss: 0.5073 - val_acc: 0.7622\n",
      "Epoch 68/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5254 - acc: 0.7558 - val_loss: 0.5090 - val_acc: 0.7633\n",
      "Epoch 69/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5248 - acc: 0.7556 - val_loss: 0.5076 - val_acc: 0.7611\n",
      "Epoch 70/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5234 - acc: 0.7561 - val_loss: 0.5191 - val_acc: 0.7590\n",
      "Epoch 71/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5242 - acc: 0.7561 - val_loss: 0.5076 - val_acc: 0.7626\n",
      "Epoch 72/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5238 - acc: 0.7559 - val_loss: 0.5119 - val_acc: 0.7587\n",
      "Epoch 73/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5238 - acc: 0.7557 - val_loss: 0.5133 - val_acc: 0.7608\n",
      "Epoch 74/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5240 - acc: 0.7562 - val_loss: 0.5116 - val_acc: 0.7606\n",
      "Epoch 75/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5236 - acc: 0.7563 - val_loss: 0.5131 - val_acc: 0.7611\n",
      "Epoch 76/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5229 - acc: 0.7566 - val_loss: 0.5056 - val_acc: 0.7646\n",
      "Epoch 77/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5236 - acc: 0.7568 - val_loss: 0.5509 - val_acc: 0.7406\n",
      "Epoch 78/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5223 - acc: 0.7574 - val_loss: 0.5065 - val_acc: 0.7642\n",
      "Epoch 79/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5223 - acc: 0.7566 - val_loss: 0.5069 - val_acc: 0.7641\n",
      "Epoch 80/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5227 - acc: 0.7568 - val_loss: 0.5063 - val_acc: 0.7634\n",
      "Epoch 81/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5220 - acc: 0.7574 - val_loss: 0.5039 - val_acc: 0.7662\n",
      "Epoch 82/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5216 - acc: 0.7583 - val_loss: 0.5092 - val_acc: 0.7630\n",
      "Epoch 83/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5210 - acc: 0.7570 - val_loss: 0.5073 - val_acc: 0.7656\n",
      "Epoch 84/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5201 - acc: 0.7580 - val_loss: 0.5053 - val_acc: 0.7646\n",
      "Epoch 85/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5197 - acc: 0.7588 - val_loss: 0.5052 - val_acc: 0.7661\n",
      "Epoch 86/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5188 - acc: 0.7584 - val_loss: 0.5009 - val_acc: 0.7686\n",
      "Epoch 87/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5182 - acc: 0.7607 - val_loss: 0.5242 - val_acc: 0.7590\n",
      "Epoch 88/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5183 - acc: 0.7596 - val_loss: 0.5035 - val_acc: 0.7670\n",
      "Epoch 89/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5178 - acc: 0.7601 - val_loss: 0.5123 - val_acc: 0.7625\n",
      "Epoch 90/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5176 - acc: 0.7595 - val_loss: 0.5070 - val_acc: 0.7641\n",
      "Epoch 91/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5174 - acc: 0.7606 - val_loss: 0.5081 - val_acc: 0.7640\n",
      "Epoch 92/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5156 - acc: 0.7617 - val_loss: 0.4979 - val_acc: 0.7705\n",
      "Epoch 93/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5153 - acc: 0.7613 - val_loss: 0.5206 - val_acc: 0.7583\n",
      "Epoch 94/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5159 - acc: 0.7618 - val_loss: 0.4940 - val_acc: 0.7731\n",
      "Epoch 95/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5146 - acc: 0.7624 - val_loss: 0.4961 - val_acc: 0.7716\n",
      "Epoch 96/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5143 - acc: 0.7610 - val_loss: 0.4969 - val_acc: 0.7719\n",
      "Epoch 97/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5142 - acc: 0.7632 - val_loss: 0.4946 - val_acc: 0.7728\n",
      "Epoch 98/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5134 - acc: 0.7631 - val_loss: 0.4946 - val_acc: 0.7723\n",
      "Epoch 99/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5141 - acc: 0.7624 - val_loss: 0.5058 - val_acc: 0.7652\n",
      "Epoch 100/100\n",
      "199737/199737 [==============================] - 7s 34us/step - loss: 0.5123 - acc: 0.7631 - val_loss: 0.5085 - val_acc: 0.7637\n"
     ]
    }
   ],
   "source": [
    "h=model.fit(xtrain, ytrain, validation_split=.2,epochs=100, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-88dbd1485819>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# summarize history for accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "history=h\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
