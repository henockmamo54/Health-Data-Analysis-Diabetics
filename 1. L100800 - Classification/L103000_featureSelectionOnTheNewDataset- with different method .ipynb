{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=pd.read_csv('../_xlable4_withNa_AllColumns.txt')\n",
    "# y=pd.read_csv('../_targelable4_withNa_AllColumns.txt')\n",
    "\n",
    "# y=y[['Unnamed: 0','L100800', ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data= pd.merge(x,y, how='inner',left_on='Unnamed: 0', right_on='Unnamed: 0')\n",
    "data= pd.read_csv('../sep19SexAndAgeAddedFINAL DATASET_ver2.txt',sep=',')\n",
    "data['max']=np.min(data[['FIELD_6','FIELD_7']],axis=1)\n",
    "data=data.drop(columns=['FIELD_6','FIELD_7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=data.query('FIELD_15 !=1 and FIELD_17 !=1 and FIELD_22 != 1 and FIELD_24 != 1 and FIELD_16 != 1 and FIELD_23 != 1')\n",
    "# data=data.query('FIELD_16 != 1 and FIELD_23 != 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(466680, 405)\n"
     ]
    }
   ],
   "source": [
    "# data=data[data.SEX==0]\n",
    "\n",
    "data=data[data.FIELD_16!=1]\n",
    "data=data[data.FIELD_23!=1]\n",
    "data=data[data.FIELD_15!=1]\n",
    "data=data[data.FIELD_17!=1]\n",
    "data=data[data.FIELD_22!=1]\n",
    "data=data[data.FIELD_24!=1]\n",
    "\n",
    "# data=data[data.AGE>=40]\n",
    "# data=data[data.AGE<50]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FIELD_1</th>\n",
       "      <th>FIELD_2</th>\n",
       "      <th>COMPARE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>B_DAY</th>\n",
       "      <th>SEX</th>\n",
       "      <th>FIELD_3</th>\n",
       "      <th>FIELD_4</th>\n",
       "      <th>FIELD_5</th>\n",
       "      <th>...</th>\n",
       "      <th>S005200</th>\n",
       "      <th>S007400</th>\n",
       "      <th>S008501</th>\n",
       "      <th>S008502</th>\n",
       "      <th>S008503</th>\n",
       "      <th>S008504</th>\n",
       "      <th>S010101</th>\n",
       "      <th>S010102</th>\n",
       "      <th>max</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000003</td>\n",
       "      <td>20130913</td>\n",
       "      <td>100000320130913</td>\n",
       "      <td>46.0</td>\n",
       "      <td>670212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1000003</td>\n",
       "      <td>20140613</td>\n",
       "      <td>100000320140613</td>\n",
       "      <td>47.0</td>\n",
       "      <td>670212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1000003</td>\n",
       "      <td>20150917</td>\n",
       "      <td>100000320150917</td>\n",
       "      <td>48.0</td>\n",
       "      <td>670212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1000003</td>\n",
       "      <td>20160617</td>\n",
       "      <td>100000320160617</td>\n",
       "      <td>49.0</td>\n",
       "      <td>670212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1000003</td>\n",
       "      <td>20170608</td>\n",
       "      <td>100000320170608</td>\n",
       "      <td>50.0</td>\n",
       "      <td>670212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 406 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  FIELD_1   FIELD_2          COMPARE   AGE     B_DAY  SEX  \\\n",
       "0           0  1000003  20130913  100000320130913  46.0  670212.0  0.0   \n",
       "1           1  1000003  20140613  100000320140613  47.0  670212.0  0.0   \n",
       "2           2  1000003  20150917  100000320150917  48.0  670212.0  0.0   \n",
       "3           3  1000003  20160617  100000320160617  49.0  670212.0  0.0   \n",
       "4           4  1000003  20170608  100000320170608  50.0  670212.0  0.0   \n",
       "\n",
       "  FIELD_3 FIELD_4 FIELD_5  ... S005200  S007400 S008501 S008502 S008503  \\\n",
       "0     NaN       4       1  ...     NaN      NaN     NaN     NaN     NaN   \n",
       "1     NaN       4       1  ...     NaN      NaN     NaN     NaN     NaN   \n",
       "2       1       4       1  ...     NaN      NaN     NaN     NaN     NaN   \n",
       "3     NaN       4     NaN  ...     NaN      NaN     NaN     NaN     NaN   \n",
       "4     NaN       4     NaN  ...     NaN      NaN     NaN     NaN     NaN   \n",
       "\n",
       "   S008504  S010101  S010102  max  CLASS  \n",
       "0      NaN     18.0     14.0  NaN      1  \n",
       "1      NaN     15.0     16.0  NaN      1  \n",
       "2      NaN     16.0     16.0  NaN      1  \n",
       "3      NaN     16.0     18.0  NaN      1  \n",
       "4      NaN     16.0     18.0  NaN      1  \n",
       "\n",
       "[5 rows x 406 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=data.copy()\n",
    "conditions3 = [\n",
    "    (df.L100800 < 100)  ,\n",
    "    (df.L100800 >= 100) & (df.L100800 < 126),\n",
    "    (df.L100800 >= 126)]\n",
    "choices3 = [0,1,2]\n",
    "df['CLASS'] = np.select(conditions3, choices3, default=0)\n",
    "data=df.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split column types to categorical and numerical\n",
    "numerical_cols = list(data.columns[~data.columns.str.startswith('FIELD')])\n",
    "categorical_cols = list(data.columns[data.columns.str.startswith('FIELD')])\n",
    "categorical_cols.append('CLASS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlation Value - for numerical values\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>CLASS</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>L501600</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>L502000</td>\n",
       "      <td>0.956054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>L303204</td>\n",
       "      <td>0.774597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>L303201</td>\n",
       "      <td>0.774597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Col       val\n",
       "277    CLASS  1.000000\n",
       "149  L501600  1.000000\n",
       "153  L502000  0.956054\n",
       "118  L303204  0.774597\n",
       "115  L303201  0.774597"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr=data[numerical_cols].corr()\n",
    "corr=corr.L100800_y\n",
    "corrvalPD=pd.DataFrame()\n",
    "corrvalPD['Col']=corr.index\n",
    "corrvalPD['val']=abs(corr.values)\n",
    "\n",
    "corrvalPD=corrvalPD.sort_values(by='val',ascending=False)\n",
    "corrvalPD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of none NA values of feature set\n",
    "mydataset=data.copy()[numerical_cols]\n",
    "colCount=[]\n",
    "for i in mydataset.columns:\n",
    "    colCount.append([i,mydataset[i].dropna().shape[0]])\n",
    "\n",
    "colCountPD=pd.DataFrame(colCount,columns=['Col','Count'])\n",
    "colCountPD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedColCorrCount=pd.merge(corrvalPD,colCountPD,how='inner',left_on='Col',right_on='Col')\n",
    "mergedColCorrCount=mergedColCorrCount[mergedColCorrCount.Count>50000]\n",
    "mergedColCorrCount=mergedColCorrCount.sort_values(by='val', ascending=False)\n",
    "mergedColCorrCount.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(mergedColCorrCount.head(15).Col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anova test - For categorical values [Questionnaire answers]\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset=data.copy()[categorical_cols] #[np.insert(categorical_cols,0,'L100800')]\n",
    "\n",
    "# mydataset=mydataset.drop(columns=['FIELD_1','FIELD_2','FIELD_8','FIELD_10','FIELD_11','FIELD_12','FIELD_39','FIELD_88',\n",
    "#                            'FIELD_89','FIELD_109','FIELD_110','FIELD_111','FIELD_70','FIELD_82','FIELD_85','FIELD_91',\n",
    "#                            'FIELD_103','FIELD_106'])\n",
    "\n",
    "# mydataset=mydataset.drop(columns=['FIELD_1','FIELD_2','FIELD_87','FIELD_8','FIELD_10','FIELD_11','FIELD_12','FIELD_39',\n",
    "#                                   'FIELD_88','FIELD_89', 'FIELD_108','FIELD_109','FIELD_110','FIELD_111','FIELD_82',\n",
    "#                                  'FIELD_91','FIELD_103','FIELD_118','FIELD_119','FIELD_120','FIELD_121','FIELD_122',\n",
    "#                                  'FIELD_123','FIELD_124','FIELD_125','FIELD_126','FIELD_127','FIELD_128','FIELD_129',\n",
    "#                                  'FIELD_130','FIELD_131','FIELD_132','FIELD_133','FIELD_134','FIELD_135','FIELD_136',\n",
    "#                                   'FIELD_137','FIELD_138','FIELD_139','FIELD_140'])\n",
    "\n",
    "\n",
    "mydataset=mydataset.drop(columns=['FIELD_1','FIELD_2','FIELD_87','FIELD_8','FIELD_10','FIELD_11','FIELD_12','FIELD_39',\n",
    "                                  'FIELD_88','FIELD_89', 'FIELD_108','FIELD_109','FIELD_110','FIELD_111','FIELD_82',\n",
    "                                 'FIELD_91','FIELD_103','FIELD_118','FIELD_119','FIELD_120','FIELD_121','FIELD_122',\n",
    "                                 'FIELD_123','FIELD_124','FIELD_125','FIELD_126','FIELD_127','FIELD_128','FIELD_129',\n",
    "                                 'FIELD_130','FIELD_131','FIELD_132','FIELD_133','FIELD_134','FIELD_135','FIELD_136',\n",
    "                                  'FIELD_137','FIELD_138','FIELD_139','FIELD_140',\n",
    "                                  'FIELD_64','FIELD_65','FIELD_66','FIELD_67','FIELD_68','FIELD_69','FIELD_70','FIELD_72',\n",
    "                                  'FIELD_73','FIELD_74','FIELD_75','FIELD_76','FIELD_77','FIELD_80','FIELD_81','FIELD_84',\n",
    "                                  'FIELD_85','FIELD_90','FIELD_93','FIELD_94','FIELD_95','FIELD_96','FIELD_97','FIELD_98',\n",
    "                                  'FIELD_101','FIELD_102','FIELD_105','FIELD_106','FIELD_112','FIELD_113','FIELD_114',\n",
    "                                  'FIELD_115','FIELD_116','FIELD_117','FIELD_118','FIELD_119','FIELD_9'\n",
    "                                 ])\n",
    "\n",
    "\n",
    "mydataset=mydataset.drop(mydataset[mydataset.FIELD_3=='`'].index)\n",
    "mydataset=mydataset.drop(mydataset[mydataset.FIELD_3=='G'].index)\n",
    "\n",
    "\n",
    "mydataset=mydataset.drop(mydataset[mydataset.FIELD_4=='.'].index)\n",
    "mydataset=mydataset.drop(mydataset[mydataset.FIELD_5=='.'].index)\n",
    "\n",
    "mydataset=mydataset.drop(mydataset[mydataset.FIELD_35=='?'].index)\n",
    "mydataset=mydataset.drop(mydataset[mydataset.FIELD_35=='.'].index)\n",
    "mydataset=mydataset.drop(mydataset[mydataset.FIELD_36=='.'].index)\n",
    "mydataset=mydataset.drop(mydataset[mydataset.FIELD_37=='\\\\'].index)\n",
    " \n",
    "# mydataset=mydataset.drop(mydataset[mydataset.FIELD_39 == '7+' ].index)\n",
    "# mydataset=mydataset.drop(mydataset[mydataset.FIELD_39 == '5~7'].index)\n",
    "# mydataset=mydataset.drop(mydataset[mydataset.FIELD_39 == '3-4'].index)\n",
    "\n",
    "\n",
    "# mydataset=mydataset.drop(mydataset[mydataset.FIELD_95=='.'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colslist=mydataset.columns\n",
    "DiabeticColVals=data.L100800_y\n",
    "\n",
    "cor=[]\n",
    "pval=[]\n",
    "count=[]\n",
    "\n",
    "for i in range(len(colslist)):\n",
    "    print(i,colslist[i])\n",
    "    \n",
    "    temp=pd.DataFrame([])\n",
    "    temp['a']=DiabeticColVals\n",
    "    temp['b']=mydataset[colslist[i]].astype(float)\n",
    "    temp=temp.dropna()\n",
    "    count.append(temp.shape[0])\n",
    "    \n",
    "    uniquevalues=temp.b.unique()\n",
    "    \n",
    "    selectedGroupVals=[]\n",
    "    for k in range(len(uniquevalues)):\n",
    "        selectedGroupVals.append(np.asarray(temp[temp['b']==uniquevalues[k]].a))\n",
    "\n",
    "    F, p = stats.f_oneway(*selectedGroupVals)\n",
    "    cor.append(F)\n",
    "    pval.append(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FvalPvalCorr=pd.DataFrame()    \n",
    "FvalPvalCorr['Cols']=colslist\n",
    "FvalPvalCorr['F']=cor\n",
    "FvalPvalCorr['P']=pval\n",
    "FvalPvalCorr['Count']=count\n",
    "FvalPvalCorr=FvalPvalCorr.sort_values(by='F', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FvalPvalCorr[FvalPvalCorr.Count>50000].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(FvalPvalCorr[FvalPvalCorr.Count>50000].Cols)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================= LASSO method\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedcols=['L100800_y', 'L100800_x', 'L104600', 'S000300', 'L103000', 'L103300', 'SEX', 'S000501', 'L101700', \n",
    "              'L190400', 'S000502', 'L190500', 'L101300', 'L190300', 'AGE','FIELD_33', 'FIELD_38', 'FIELD_31', \n",
    "              'FIELD_40', 'FIELD_27', 'FIELD_29', 'FIELD_41', 'FIELD_21', 'FIELD_5', 'FIELD_14'\n",
    "             ]\n",
    "data2=data[selectedcols].copy().dropna()\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data2[['L100800_x', 'L104600', 'S000300', 'L103000', 'L103300', 'SEX', 'S000501', 'L101700', \n",
    "              'L190400', 'S000502', 'L190500', 'L101300', 'L190300', 'AGE','FIELD_33', 'FIELD_38', 'FIELD_31', \n",
    "              'FIELD_40', 'FIELD_27', 'FIELD_29', 'FIELD_41', 'FIELD_21', 'FIELD_5', 'FIELD_14']]\n",
    "y=data2.L100800_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(x,y)\n",
    "\n",
    "print(clf.coef_)\n",
    "\n",
    "print(clf.intercept_)  \n",
    "\n",
    "# pd.DataFrame([x.columns.ravel(),clf.coef_.ravel()],columns=['Name','coeff'])\n",
    "\n",
    "lassod=pd.DataFrame()\n",
    "lassod['Name']=x.columns\n",
    "lassod['coeff']=clf.coef_\n",
    "lassod\n",
    "\n",
    "lassod=lassod.drop(lassod[lassod.coeff==0].index)\n",
    "lassod.coeff=abs(lassod.coeff)\n",
    "lassod=lassod.sort_values(by='coeff', ascending=False)\n",
    "lassod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(lassod.Name[:16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================== SelectKBest method\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=10)\n",
    "fit = bestfeatures.fit(x,y)\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(x.columns)\n",
    "\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "\n",
    "# x=x[x.columns[:15]]\n",
    "print(list(x.columns[:16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================= LinearSVC\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(x, y)\n",
    "# model = SelectFromModel(lsvc, prefit=True)\n",
    "# X_new = model.transform(x)\n",
    "# X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================ SequentialFeatureSelector \n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "# from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# lr = RandomForestRegressor()\n",
    "\n",
    "# sfs = SFS(lr, \n",
    "#           k_features=13, \n",
    "#           forward=True, \n",
    "#           floating=False, \n",
    "#           scoring='neg_mean_squared_error',\n",
    "#           cv=5)\n",
    "\n",
    "# sfs = sfs.fit(x, y)\n",
    "# fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')\n",
    "\n",
    "# plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================= feature_importances\n",
    "====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(x,y)\n",
    "\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=x.columns)\n",
    "feat_importances.nlargest(16).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "print(feat_importances.nlargest(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================= Backward Elimination\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.regression.linear_model as sm\n",
    "temp=x.copy()\n",
    "temp['const']=np.ones((x.shape[0],1))\n",
    "regressor_OLS = sm.OLS(endog = y, exog = temp).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=temp.drop(columns=['L103000','L103300','L190300','FIELD_40','FIELD_27','FIELD_29','FIELD_21','FIELD_5','FIELD_14'])\n",
    "regressor_OLS = sm.OLS(endog = y, exog = temp).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=temp.drop(columns=['FIELD_33'])\n",
    "regressor_OLS = sm.OLS(endog = y, exog = temp).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
